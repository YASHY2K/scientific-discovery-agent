{
  "paper_id": "2310.01324",
  "source": "arxiv",
  "url": "https://arxiv.org/abs/2310.01324",
  "extracted_date": "2025-09-18T05:26:11.102454",
  "pdf_url": "https://arxiv.org/pdf/2310.01324.pdf",
  "total_pages": 25,
  "pages": [
    {
      "page_number": 1,
      "content": "ZeroI2V: Zero-Cost Adaptation of Pre-trained\nTransformers from Image to Video\nXinhao Li1,2, Yuhan Zhu1, and Limin Wang1,2â‹†\n1State Key Laboratory for Novel Software Technology, Nanjing University\n2Shanghai AI Laboratory\nxinhaoli00@outlook.com zyuhan0812@gmail.com lmwang@nju.edu.cn\nhttps://github.com/MCG-NJU/ZeroI2V\nAbstract. Adapting image models to the video domain has emerged as\nan efficient paradigm for solving video recognition tasks. Due to the huge\nnumber of parameters and effective transferability of image models, per-\nforming full fine-tuning is less efficient and even unnecessary. Thus, recent\nresearch is shifting its focus toward parameter-efficient image-to-video\nadaptation. However, these adaptation strategies inevitably introduce\nextra computational costs to deal with the domain gap and temporal\nmodeling in videos. In this paper, we present a new adaptation paradigm\n(ZeroI2V) to transfer the image transformers to video recognition tasks\n(i.e., introduce zero extra cost to the original models during inference).\nTo achieve this goal, we present two core designs. First, to capture the\ndynamics in videos and reduce the difficulty of image-to-video adaptation,\nwe exploit the flexibility of self-attention and introduce spatial-temporal\ndual-headed attention (STDHA). This approach efficiently endows the\nimage transformers with temporal modeling capability at zero extra pa-\nrameters and computation. Second, to handle the domain gap between\nimages and videos, we propose a linear adaption strategy that utilizes\nlightweight densely placed linear adapters to fully transfer the frozen\nimage models to video recognition. Thanks to the customized linear de-\nsign, all newly added adapters could be easily merged with the original\nmodules through structural reparameterization after training, enabling\nzero extra cost during inference. Extensive experiments on representative\nfully-supervised and few-shot video recognition benchmarks showcase\nthat ZeroI2V can match or even outperform previous state-of-the-art\nmethods while enjoying superior parameter and inference efficiency.\n1 Introduction\nAdapting pre-trained foundation models such as BERT [12] and GPT [5,55,56]\nthrough efficient strategies has yielded excellent performance on downstream tasks\nin natural language understanding. This new paradigm is becoming popular in\ncomputer vision due to the available pre-trained image models such as CLIP [54]\nand DINO [7,50]. These models could be easily adapted to downstream tasks\nâ‹†Corresponding author.arXiv:2310.01324v2  [cs.CV]  11 Jul 2024",
      "char_count": 2568
    },
    {
      "page_number": 2,
      "content": "2 X. Li et al.\nImage ViTBlock\nMLP\nNorm\nSpatial -Temporal\nDual -Headed \nAttention\nLinear Adapter\nNormLinear Adapter\nLinear Adapter\nLinear Adapter\nVideo ViTBlockImprove MHSA\n& Insert Adapters\nStructural\nReparameterization\nMLP\nNorm\nMulti -Head \nSelf-Attention\nNorm\nSTDHAMLP\nNormNormWrapped \nwith \nlinear \nAdaptersWrapped \nwith \nlinear \nAdapters\nTrainable Frozen\nFig.1:Left :Ourproposedimage-to-videotransferlearningmethod. Right:Comparison\nof PETL mehods on SSv2 validation set. For a more intuitive comparison, the views\nof the methods in the figure are all 8 Ã—3Ã—1. Two core techniques enable us to achieve\nsuperior performance on video tasks without introducing additional computation and\nparameters during inference.\nthrough linear probes, fine-tuning, or even zero-shot recognition, exhibiting\nrobustness and strong transfer capabilities similar to those of large-scale language\nmodels. Recently, parameter-efficient transfer learning (PETL) [9,25,40,49,51,83]\nis becoming an efficient paradigm to adapt these large pre-trained models due to\ntheir huge numbers of parameters and high computational cost of full fine-tuning.\nFor video understanding, there exist several large pre-trained video models [60,\n63] from self-supervised learning, but these models are of high computational\ncomplexity due to the joint spatiotemporal attentions. Therefore, adapting pre-\ntrained image models to the video domain through efficient strategies is still a\npractical solution to video recognition. In fact, the state-of-the-art video networks\nhavelongreliedonthepre-trainedimagemodelsbyinflatingthekernels[1,8,41,44]\nor inserting plug-and-play temporal modules [35,39,45,64,65]. However, most of\nthese methods necessitate full fine-tuning, which involves updating all the model\nparameters during training on video datasets. As the scale of pre-trained models\nincreases, full fine-tuning becomes impractical due to the high training costs and\nthe risk of overfitting or even catastrophic forgetting when the downstream data\nis limited. In addition, these methods often inevitably introduce extra costs to\nthe adapted video models due to these newly added modules.\nIn this paper, we aim to present a new efficient paradigm of adapting image\ntransformers to video downstream tasks with two main objectives. First, inspired\nbythePETLmethodsinNLP[23,24,28,33]andimageunderstanding[9,25,49],we\naim to devise a parameter-efficient transfer technique from image to video, which\ncan effectively reduce the risk of over-fitting and greatly improve the training\nefficiency. Second, to overcome the issue of high computation in the adapted\nvideo models, we try to present a new adaptation method without introducing\nany extra computations to the final video models during inference. This zero\nextra inference cost adaptation would allow for more efficient deployment of\ntransferred video models in real applications.",
      "char_count": 2892
    },
    {
      "page_number": 3,
      "content": "ZeroI2V 3\nTo achieve the above two objectives, we propose a novel transfer learning\nmethod (as shown in Figure 1) that can utilize the off-the-shelf pre-trained image\ntransformers to achieve excellent performance on video tasks without\nadditional parameters and computation during inference . To be specific,\nfor the temporal modeling required for video tasks, we transform multi-head self-\nattention into spatio-temporal dual-head attention (STDHA) by reassigning some\nheads to achieve temporal modeling at zero computation and zero parameters.\nFor image-to-video transfer, we explore the strategy of using linear adapters to\nfully adapt the parameters of each part of the model and merge them with the\nfrozen original parameters through structural reparameterization after training,\nthus achieving zero extra cost during inference.\nTo summarize, we make the following contributions: 1)We propose a new\napproach for parameter-efficient image-to-video transfer learning that can achieve\nthe efficient adaptation of transformers from image to video without introducing\nadditional computation and parameters during inference. 2)We introduce a novel\nattention mechanism named Spatial-Temporal Dual-Headed Attention (STDHA),\nwhich utilizes the flexibility of self-attention to achieve temporal modeling without\nintroducing extra computation and parameters. 3)To the best of our knowledge,\nwe are the first to investigate the achievement of zero extra inference cost image-\nto-video adaptation through the utilization of a linear structure. We establish\nan empirical study by conducting extensive experiments with a diverse range\nof adaptation strategies. 4)Our method achieves comparable or even better\nperformance than state-of-the-art methods on popular fully-supervised and few-\nshot video recognition benchmarks while enjoying the advantage of parameter\nand inference efficiency.\n2 Related work\nPre-trained image transformers The powerful scalability of ViT [13] brings\nmore possibilities to the pre-trained image model. In addition to the traditional\nsupervised approach [13,42,77], recent works [3,7,20,21,50] utilize self-supervised\nlearningtoeffectivelylearnrepresentationsfromunlabeleddata.Moreover,several\nworks [10,29,54,61] adopt large-scale multi-modal data ( e.g., text-image pairs) to\nlearn visual representations with great transferability. Our proposed adaptation\nstrategy can leverage these off-the-shelf pre-trained image transformers to achieve\noutstanding performance on video tasks.\nVideo action recognition is crucial for downstream tasks [59,84]. Traditionally,\nstate-of-the-art methods have long relied on image models. Previous works for\naction recognition can be classified into two categories: one is to extend the image\nmodel for spatial-temporal modeling by inflating weights and structures [8,14,\n16,17,30,36,44], while the other is to directly utilize the image model as the\nbackbone and insert plug-and-play modules for temporal modeling [39,45,64,65,\n82]. Following the success of new training paradigms in image understanding,\nseveral works have attempted to learn transferable video representations via self-\nsupervised learning [46,60,63,67] or multi-modal video-text pre-training [31,32,",
      "char_count": 3221
    },
    {
      "page_number": 4,
      "content": "4 X. Li et al.\n48,66]. However, the above methods usually require full fine-tuning of the entire\nmodel or training from scratch, resulting in high training costs and additional\ncomputational overhead. In this work, we avoid the above problems by adapting\nthe pre-trained image transformers to video tasks in an efficient manner.\nParameter-efficient transfer learning To address the issue of training in-\nefficiency caused by the continuous growth of model size, Parameter-efficient\ntransfer learning (PETL) is initially introduced in NLP [23,24,28,33,52,53,76]\nand subsequently applied to vision tasks [9,22,25,38,49,72,73,83]. These tech-\nniques aim to achieve comparable or even superior performance on other tasks\nby fine-tuning only a small subset of trainable parameters. Most PETL meth-\nods [9,22,25,38,49,80,83] in vision domain are limited to transfer within the\nsame modality ( e.g., image-to-image or video-to-video). In contrast, our research\nfocuses on image-to-video transfer learning. Despite progress made by recent\nstudies [40,51,75], these methods require additional computation and parameters\nfor temporal modeling of video tasks and image-to-video adaptation. For exam-\nple, EVL [40] incorporates an additional temporal transformer decoder, while\nST-Adapter [51] introduces additional adapters with depth-wise 3D convolution\nlayers. Similarly, AIM [75] adds extra adapters and necessitates an additional time\nattention calculation at each block. In contrast to previous works, our proposed\nmethod eschews the introduction of additional computation or parameters during\ninference, yet still achieves comparable or superior performance compared to\nprevious methods.\n3 Methodology\nIn this section, we first briefly revisit the basic block of ViT (Sec. 3.1), and then\ndiscuss how to utilize the flexibility of self-attention to achieve temporal modeling\nwithout introducing additional computation and parameters (Sec. 3.2). Finally,\nwe explain how we implement zero-cost image-to-video adaptation with a serial\nlinear structure (Sec. 3.3).\n3.1 Preliminary\nThe original ViT [13] block consists of two network layers: multi-head self-\nattention (MHSA) and multi-layer perceptron (MLP). As shown in Figure 1, a\nViT block consists of MHSA and MLP connected in series in a residual structure:\nzl=xl+MHSA (LN(xl)), (1)\nxl+1=zl+MLP (LN(zl)), (2)\nwhere LN denotes layer normalization [2] and xlrepresents the input to the\nl-th ViT block. We review their specific implementation details. For the sake of\nsimplicity, we ignore the bias and denote XâˆˆRnÃ—das input of MHSA and MLP.\nMHSA first performs three different linear projections WQ\nattn, WK\nattn, WV\nattnâˆˆ\nRdÃ—don the input Xto obtain the query Qand key-value pairs K, V. These are",
      "char_count": 2740
    },
    {
      "page_number": 5,
      "content": "ZeroI2V 5\nğ‘¾ğ‘¾ğ®ğ®ğ®ğ®\nğ‘¾ğ‘¾ğğğğğğğğğ‘¾ğ‘¾ğ€ğ€ğğğ€ğ€ğ®ğ®ğ€ğ€ğ€ğ€ğ€ğ€ğ‘¾ğ‘¾ğğğ¨ğ¨ğğ\nğ‘¾ğ‘¾ğğğ¨ğ¨ğğ\nğ‘¾ğ‘¾ğğğ€ğ€ğğ\nğ‘¾ğ‘¾ğ€ğ€ğ€ğ€ğ€ğ€ğğğ‘½ğ‘½ğ‘¾ğ‘¾ğ€ğ€ğ€ğ€ğ€ğ€ğğğ‘²ğ‘²ğ‘¾ğ‘¾ğ€ğ€ğ€ğ€ğ€ğ€ğğğ‘¸ğ‘¸Concatğ‘¾ğ‘¾ğ€ğ€ğ€ğ€ğ€ğ€ğğğ‘¶ğ‘¶\nScaled Dot -Product \nAttention\nğ’‰ğ’‰heads\nHeads from\nthe current frameHeads from\nthe other frames\nDuring Training During InferenceScaled Dot -Product \nAttention\n(a) Layer merging via reparameterization (b) Spatial-temporal dual-headed attention\nFig.2: Illustration of the proposed linear adaptation and STDHA.\nthen evenly divided into hheads by channel. Each head independently performs\nthe scaled dot-product attention calculation. Finally, the heads are concatenated\nby channel and then a linear projection WO\nattnâˆˆRdÃ—dis performed to obtain the\nfinal calculation result:\nQ, K, V =XWQ\nattn, XWK\nattn, XWV\nattn, (3)\nhead i=Attention (Qi, Ki, Vi), (4)\nMHSA (X) =Concat (head 1,Â·Â·Â·,head h)WO\nattn. (5)\nMLP involves two linear projections Wup\nmlpâˆˆRdÃ—dâ€², Wdown\nmlpâˆˆRdâ€²Ã—d, dâ€²> dand\none non-linear activation function Ïƒ:\nMLP (X) =Ïƒ(XWup\nmlp)Wdown\nmlp. (6)\n3.2 Zero-Cost temporal modeling\nApplying image models to video tasks often requires the incorporation of ad-\nditional modules for temporal modeling, which not only introduces additional\nparameters and computation, but also results in additional training costs. In this\nwork, we address temporal modeling from three key perspectives: (1) Capability\nof capturing the temporal dynamics. (2) Reducing the difficulty of image-to-\nvideo adaptation. (3) Minimizing the introduction of additional computation and\nparameters compared to the original model. [47] suggests that most heads are\nredundant given the rest of the model. Inspired by this, we attempt to reassign\nsome heads as temporal heads in the multi-head attention to perform temporal\nmodeling tasks, while the remaining heads continue to perform spatial modeling\ntasks asspatial heads , thereby achieving efficient spatial-temporal modeling.\nSpatial-temporal dual-headed attention (STDHA) As shown in Figure\n2b, consider an input sequence X={x1, x2,Â·Â·Â·, xT}where xtâˆˆRnÃ—d. Let\nthe query and key-value pairs obtained after the linear projection of the xtbe\nQt, Kt, VtâˆˆRnÃ—d. We divide the hheads of the MHSA into two groups of",
      "char_count": 2151
    },
    {
      "page_number": 6,
      "content": "6 X. Li et al.\nsizehâˆ’kandk. One group of heads queries the key-value pairs at the current\ntime tto perform spatial modeling , while the other group of heads queries the\nkey-value pairs at other times t+âˆ†tito perform temporal modeling . Finally, the\ninformation from the two groups of heads is aggregated by a linear projection to\nperformspatial-temporal modeling :\nS-head i=Attention (Qt\ni, Kt\ni, Vt\ni), (7)\nT-head i=Attention (Qt\ni, Kt+âˆ†ti\ni , Vt+âˆ†ti\ni )(âˆ†tiÌ¸= 0), (8)\nSTDHA (X) =Concat (T-head 1,Â·Â·Â·,T-head k,S-head k+1Â·Â·Â·S-head h)WO\nattn,(9)\nwhere âˆ†tirepresents the time offset of the key-value pair of the i-th head. We\ndid not directly use temporal attention or temporal convolution for the temporal\nmodeling like previous works [40,51,75]. Instead, we design a more efficient\nspatiotemporal modeling operator by decoupling spatial modeling and temporal\nmodeling to different heads:\nâ€“For the spatial head, it still only needs to complete the spatial modeling task\nas the original image transformer, which reduces the difficulty of achieving\nimage-to-video adaptation.\nâ€“For the temporal head, it actually implements the inter-frame attention\nmechanism with frames at different times. [78] have demonstrated the\neffectiveness of an inter-frame attention mechanism for modeling motion\ninformation, which is crucial for action recognition tasks. In addition, as\nshown in Table 1c, we can achieve both short-distance and long-distance\nmodeling by controlling the âˆ†tiof the temporal head, which enables us to\nachieve enhanced temporal modeling capabilities.\nComparison with other zero-cost operators There have been several previ-\nous attempts [6,70,79] to use image transformers to achieve efficient temporal\nmodeling at zero parameters and zero computation. For example, [6] achieves\napproximations to full space-time attention by mixing tokens from adjacent\nframes. [79] performs temporal modeling by using channel shift on the cls tokens\nof different frames. [70] mixes information from adjacent frames using temporal\npatch shift and temporal channel shift before MHSA. However, these methods do\nnot take advantage of the inherent characteristics of the transformer structure. By\ndecoupling the learning of spatial and temporal information with head relocation,\nSTDHA maintains the purity of key-value pair information within the\nsame head, thereby achieving better spatial-temporal information learning than\nother zero-cost temporal modules. And STDHA simultaneously captures both\nshort-range and long-range dependencies , rather than being limited to\nadjacent frames. As shown in Table 1, these two key distinctions enable our\nSTDHA to achieve superior spatial-temporal modeling.\n3.3 Zero Extra Inference Cost image-to-video adaptation\nInspired by LoRA [24], we can fine-tune the model using a linear structure and\nthen merge it with the original model during inference. However, to deal with",
      "char_count": 2897
    },
    {
      "page_number": 7,
      "content": "ZeroI2V 7\nthe domain gap between images and videos, previous works [40,51,75] often use\nnonlinear structures to achieve stronger transfer capabilities. Therefore, we need\nto further consider how to achieve effective image-to-video transfer using only a\nlinear structure.\nLayer merging via structural reparameterization LetWoldrepresent the\nfrozen weights of the original model, and Wnewrepresent the new trainable\nweights. Reviewing the structure of LoRA, it uses a low-rank decomposition\nmatrix WLoRAparallel to the original weights:\nWnew=WLoRA +Wold=WupWdown +Wold. (10)\nIn this work, we use a serial linear structure called Linear Adapter to fine-tune\nthe original parameters. As shown in Figure 2a, we use structural reparameteri-\nzation to perform layer merging after training:\nWnew=WAdapter Wold= (I+WupWdown)Wold, (11)\nwhere Iis the identity matrix, WupâˆˆRmÃ—k, WdownâˆˆRkÃ—n, bottleneck width\nkâ‰ªmin(m, n). As seen in Table 3, compared to parallel structures, serial\nstructures can be more flexibly inserted into the network structure ( e.g., for\nnon-square matrices, under the same bottleneck dimension, using LoRA requires\na larger number of parameters compared to Linear Adapter), which endows it\nwith better transfer capabilities.\nFull adaptation with densely placed linear adapters By observing the\nstructure of MHSA and MLP, we can see that all their trainable parameters\nconcentrate on the linear projections at both ends of the structure. Therefore,\nfine-tuning the model essentially updates these linear projections. Previous\nworks [51,75] often selectively tune part of the parameters ( e.g., placing only\nan adapter before MHSA) instead of tuning all parameters to avoid excessive\nadditional computational and parameter costs, while we can achieve zero-cost full\nadaptation by tuning all parameters through wrapping MHSA and MLP with\nlinear adapters. Table 3 shows that full adaptation enables us to achieve excellent\nimage-to-video transfer performance with a linear structure, compensating for\nthe performance degradation caused by the removal of nonlinearity.\n4 Experiments\n4.1 Experiments setup\nWe evaluate our method on five widely-used video recognition benchmarks: two\nlarge-scale datasets, namely Kinetics-400 (K400) [8] and Something-Something V2\n(SSv2)[18],inadditiontothreesmaller-scaledatasets,UCF101[58],HMDB51[27]\nand Diving48 [37]. We also evaluate our method on action detection dataset\nAVA [19]. This diverse dataset selection allows for a comprehensive evaluation of\nour model across various scales and domains. The specific model configuration and\ntraining strategy can be found in the supplementary. For most main experiments,\nwe use ViT-B and ViT-L pre-trained by CLIP [54] as our backbone models.",
      "char_count": 2731
    },
    {
      "page_number": 8,
      "content": "8 X. Li et al.\nTable 1: Ablation study on STDHA. Most of the symbols in the table have been\ndeclared in the methodology section 3. (a) Rcdenotes channel change ratio, \"Shiftâ€™ refers\nto temporal channel shift, while \"HR\" denotes head relocation as used by STDHA. (b)\nWe use a multiset to represent the time offsets of different heads ( e.g., \"1Â·2\" means\nthat there are 2 heads with âˆ†t= 1). When âˆ†t=0, it represents a spatial head. (c)\n\"Temporal RF\" refers to temporal receptive field of a single STDHA.\nRc Method Top-1\n1/6[cls] token shift 61.4\nShift QKV 64.5\nShift KV 64.6\nHRQKV 64.8\nHRKV(STDHA) 66.0\n1/4Shift KV 64.0\nHRKV(STDHA) 65.8\n(a)Comparisonoftemporalmodelingmethods,Backbone âˆ†tof heads kTop-1\nViT-B\n(h=12){1Â·1\n2,âˆ’1Â·1\n2,0Â·11}1 64.8\n{1Â·1,âˆ’1Â·1,0Â·10}266.0\n{1Â·2,âˆ’1Â·2,0Â·8}4 65.6\n{1Â·3,âˆ’1Â·3,0Â·6}6 65.6\nViT-L\n(h=16){1Â·1,âˆ’1Â·1,0Â·14}2 67.7\n{1Â·2,âˆ’1Â·2,0Â·12}468.5\n{1Â·3,âˆ’1Â·3,0Â·10}6 68.3\n(b)Effect of the number of temporal heads\nFrames âˆ†tof heads Temporal RF Top-1\n8{1Â·1,0Â·11} 2 64.7\n{1Â·1,âˆ’1Â·1,0Â·10} 3 66.0\n{1Â·1,âˆ’1Â·1,2Â·1,0Â·9} 4 65.5\n{1Â·1,âˆ’1Â·1,2Â·1,âˆ’2Â·1,0Â·8} 5 65.7\n16{1Â·1,âˆ’1Â·1,0Â·10} 3 67.2\n{1Â·1,âˆ’1Â·1,2Â·1,0Â·9} 4 67.3\n{1Â·1,âˆ’1Â·1,2Â·1,âˆ’2Â·1,0Â·8} 5 67.8\n{1Â·1,âˆ’1Â·1,2Â·1,âˆ’2Â·1,3Â·1,0Â·7} 6 67.6\n{1Â·1,âˆ’1Â·1,2Â·1,âˆ’2Â·1,3Â·1,âˆ’3Â·1,0Â·6} 7 67.3\n32{1Â·1,âˆ’1Â·1,0Â·10} 3 67.3\n{1Â·1,âˆ’1Â·1,2Â·1,0Â·9} 4 67.8\n{1Â·1,âˆ’1Â·1,2Â·1,âˆ’2Â·1,0Â·8} 5 68.5\n{1Â·1,âˆ’1Â·1,2Â·1,âˆ’2Â·1,3Â·1,0Â·7} 6 68.6\n{1Â·1,âˆ’1Â·1,2Â·1,âˆ’2Â·1,3Â·1,âˆ’3Â·1,0Â·6} 7 68.4\n{1Â·1,âˆ’1Â·1,2Â·1,âˆ’2Â·1,3Â·1,âˆ’3Â·1,4Â·1,0Â·5} 8 68.2\n(c)Effect of temporal receptive field at at different input lengths.\n4.2 Ablation study\nTo validate the effectiveness of our method on image-to-video transfer and\ntemporal modeling, we first conduct ablation experiments on the SSv2 dataset.\nAll ablation experiments were performed using ViT-B/16 with 8 input frames\nunless specified.\nEffectiveness of STDHA Table 1a compares STDHA with other zero-cost\ntemporal modeling methods. The [cls] token shift is implemented according to\nthe original paper [79], with [cls] token shift performed before MHSA and MLP.\nThe temporal channel shift operation refers to TPS [70], which shifts a portion of\nthe channels for each head. It can be seen that STDHA significantly outperforms\nother methods at the same channel change ratio, demonstrating the importance\nof preserving the purity of information within each head.\nEffect of the number of temporal heads and temporal receptive field\nWe examined the influence of the number of temporal heads and the temporal\nreceptive field in ViT-B and ViT-L. Our findings, detailed in Tables 1b and 1c,",
      "char_count": 2492
    },
    {
      "page_number": 9,
      "content": "ZeroI2V 9\nTable3:Comparisonofadaptionstrategies. \"Width\"referstothebottleneckwidth\nof LoRA/Adapter. \"Tunable Params\" refers to extra trainable parameters besides the\nparameters of the ViT backbone and linear classifier. â€œ âœ“\" and â€ âœ—\" indicate whether the\ncorresponding weights have undergone fine-tuning, and \" âœ“\" indicates that WQ\nattn, WK\nattn\nandWV\nattnshare the same adapter. \"Latency\" refers to inference latency with 3 samples.\nAll results are obtained using a same V100-32G with PyTorch-builtin mixed precision.\nMethodWeights of ViT block Tunable\nParams(M)Bottleneck\nWidthLatency\n(ms)SSv2\nTop-1WQ\nattnWK\nattnWV\nattnWO\nattnWup\nmlpWdown\nmlp\nFull Fine-tuning âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ 86 - 28.9 63.2\nLinear Probe âœ— âœ— âœ— âœ— âœ— âœ— 0 - 28.9 20.0\nOnly tuning temporal head âœ“ âœ“ âœ“ âœ“ âœ— âœ— 4.6 - 28.9 59.6\nST-Adapter [51]âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ 14 192 41.0 66.2\nâœ“ âœ“ âœ“ âœ— âœ“ âœ— 14 384 38.8 65.8\nLoRA [24]âœ“ âœ— âœ“ âœ— âœ— âœ— 7 192\n28.964.2\nâœ“ âœ“ âœ“ âœ“ âœ— âœ— 14 192 65.0\nâœ“ âœ— âœ“ âœ— âœ“ âœ“ 25 192 64.3\nâœ“ âœ— âœ“ âœ— âœ“ âœ“ 17 128 65.6\nâœ“ âœ“ âœ“ âœ“ âœ“ âœ“ 32 192 65.0\nâœ“ âœ“ âœ“ âœ“ âœ“ âœ“ 21 128 65.5\nAdapter w/ GELUâœ“ âœ“ âœ“ âœ“ âœ“ âœ“ 7 96 37.3 65.6\nâœ“ âœ“ âœ“ âœ— âœ“ âœ— 7 192 34.9 64.6\nâœ“ âœ“ âœ“ âœ“ âœ“ âœ— 10 192 36.3 66.1\nâœ“ âœ“ âœ“ âœ“ âœ“ âœ“ 14 192 38.4 66.1\nLinear Adapter (Ours)âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ 7 96\n28.965.0\nâœ“ âœ“ âœ“ âœ— âœ“ âœ— 7 192 64.4\nâœ“ âœ“ âœ“ âœ“ âœ“ âœ— 10 192 65.2\nâœ“ âœ“ âœ“ âœ“ âœ“ âœ“ 14 192 66.0\nâœ“ âœ“ âœ“ âœ“ âœ“ âœ“ 20 192 66.3\nâœ“ âœ“ âœ“ âœ“ âœ“ âœ“ 14 128 66.2\nsuggest that the optimal proportion of temporal heads in ViT lies between 1/6\nand 1/4. For the temporal receptive field, our results indicate that for 8-frame\ninputs, a field of 3 is sufficient, while for longer inputs (16/32 frames), performance\nimproves with an increase in the field from 3, saturating at around 5 or 6. Hence,\nwe employ different STDHA configurations based on input length.\nTable2: Effect ofbottleneck\nratio of linear adapters.\nRatioTunable\nParam(M)Top-1\n0.0625 3 64.2\n0.125 7 65.0\n0.25 14 66.0\n0.5 28 65.8Effect of bottleneck ratio We set the bottle-\nneck dimension equal to the product of the ViT\nwidth and the bottleneck ratio. As shown in Ta-\nble 2, a bottleneck ratio of 0.25 achieves a good\ntrade-off between performance and the number\nof parameters. Therefore, we choose 0.25 as the\nbottleneck ratio for all subsequent experiments.\nComparison of adaptation strategies In Table 3, we compare the image-to-\nvideo transfer ability of our method with a diverse range of adaptation methods.\nFor a fair comparison, we all use STDHA with the same setting to provide\ntemporal modeling capabilities. From the results, we can observe that:\nâ€“Even with minimal parameters being fine-tuned, our Linear Adapter signifi-\ncantly outperforms full fine-tuning (66.3 vs 63.2). Despite updating the fewest\nparameters, the linear probe performs poorly in image-to-video transfer.",
      "char_count": 2691
    },
    {
      "page_number": 10,
      "content": "10 X. Li et al.\nTable 4: Results on Kinetics-400 validation set . Views = #frames Ã—#spatial\ncropsÃ—#temporal clips. â€œGFLOPsâ€ means 109FLOPs, \"M\" means 106. â€œExtra GLOPsâ€\nrefers to the extra computation added to the original ViT under the same number\nof views. \"New Params\" refers to additional parameters during inference besides the\nparameters of the original ViT backbone and linear classifier.\nMethods Pretrain Views GFLOPsExtra\nGFLOPsParam(M)New\nParam(M)Top-\n1Top-\n5\nMethods with full fine-tuning\nUniFormer-B [30] IN1K 32Ã—3Ã—43108 - 50 - 83.0 95.4\nTimeSformer-L [4] IN21K 96Ã—3Ã—17140 - 121 - 80.7 94.7\nVideoSwin-L [44] IN21K 32Ã—3Ã—47248 - 197 - 83.1 95.9\nMViTv2-L( â†‘312) [36] IN21K 40Ã—5Ã—342420 - 218 - 86.1 97.0\nViViT-L/16x2 FE [1] JFT 32Ã—3Ã—111940 - 311 - 83.5 94.3\nMTV-L [74] JFT 32Ã—3Ã—418050 - 876 - 84.3 96.3\nViT-B/16 [51] CLIP 8 Ã—1Ã—3 422 0 86 0 81.0 95.5\nActionCLIP-B/16 [66] CLIP 32Ã—3Ã—1016893 13 142 56 83.8 97.1\nX-CLIP ViT-L/14 [48] CLIP 8 Ã—3Ã—4 7896 107 420 116 87.1 97.6\nText4Vis ViT-L/14 [69] CLIP 32Ã—3Ã—419944 - 347 43 87.1 97.4\nMethods with PETL\nVideoPrompt ViT-B/16 [26] CLIP 16Ã—5Ã—1- - - - 76.9 93.5\nST-Adapter ViT-B/16 [51] IN21K 8 Ã—1Ã—3 455 33 93 7 76.6 -\nST-Adapter ViT-L/14 [51] CLIP 32Ã—1Ã—38248 322 19 87.2 97.6\nEVL ViT-B/16 [40] IN21K 8 Ã—1Ã—3 454 32 115 29 75.4 -\nEVL ViT-L/14 [40] CLIP 8 Ã—1Ã—3 2022 76 362 58 86.3 -\nAIM ViT-B-14 [75] IN21K 8 Ã—1Ã—3 624 202 100 14 78.8 -\nAIM ViT-L/14 [75] CLIP 32Ã—1Ã—311208 3425 341 38 87.5 97.7\nZeroI2V ViT-B/16 IN21K 8 Ã—1Ã—3 422 0 86 0 78.6 -\nZeroI2V ViT-B/16 CLIP 8 Ã—1Ã—3 422 0 86 0 83.0 95.8\nZeroI2V ViT-B/16 CLIP 16Ã—1Ã—3844 0 86 0 83.4 96.2\nZeroI2V ViT-B/16 CLIP 32Ã—1Ã—31688 0 86 0 83.7 96.4\nZeroI2V ViT-L/14 CLIP 8 Ã—1Ã—3 1946 0 304 0 86.3 97.4\nZeroI2V ViT-L/14 CLIP 16Ã—1Ã—33892 0 304 0 86.8 97.6\nZeroI2V ViT-L/14 CLIP 32Ã—1Ã—37783 0 304 0 87.2 97.6\nâ€“Tuning only the temporal head achieves about 95% of the full fine-tuning\nperformance, suggesting that extensive fine-tuning of the spatial head may not\nbe necessary to attain satisfactory transfer performance due to the decoupling\nof spatial and temporal modeling reduces the difficulty of adaptation.\nâ€“OurFull Adaptation strategy is not only effective for linear adapters, but\nalso for non-linear adapters such as the ST-Adapter and GELU Adapter.\nIt not only enhances their adaptation performance, but also eliminates the\nperformance gap between linear and non-linear structures.\nâ€“Due to the inflexibility of the parallel structure, for non-square matrices like\nWmlp, LoRA requires more parameters under the same bottleneck width. It\nneeds to decrease the bottleneck width of the low-rank matrix to align it\nwith the number of parameters of the linear adapter. However, this reduction\nin bottleneck width can limit its adaptation ability, ultimately leading to\nresults that are significantly worse than those of the Linear Adapter.",
      "char_count": 2822
    },
    {
      "page_number": 11,
      "content": "ZeroI2V 11\nTable 5: Results on Something-Something v2 validation set. â€ indicates that\nthe model is pre-trained on both IN21K (except for Uniformer [30] which uses IN1K)\nand K400/K600. Other notations are the same as Table 4.\nMethods Pretrain Views GFLOPsExtra\nGFLOPsParam(M)New\nParam(M)Top-\n1Top-\n5\nMethods with full fine-tuning\nTimeSformer-L [4] IN21K 64Ã—3Ã—17140 - 121 - 62.4 -\nViViT-L [1] K400 â€ 16Ã—3Ã—411892 - 311 - 65.4 89.8\nMTV-B( â†‘320) [74] K400 â€ 32Ã—3Ã—411160 - 310 - 68.5 90.4\nVideoSwin-B [44] K400 â€ 32Ã—3Ã—1963 - 89 - 69.6 92.7\nMViTv2-L( â†‘312) [36] K400 â€ 40Ã—3Ã—18484 - 213 - 73.3 94.1\nUniFormer-B [30] K600 â€ 32Ã—3Ã—1777 - 50 - 71.2 92.8\nViT-L/14 [13] CLIP 8 Ã—3Ã—1 1946 0 304 0 48.7 77.5\nILA ViT-L/14 [62] CLIP 8 Ã—3Ã—4 10884 3100 529 225 67.8 90.5\nMethods with PETL\nST-Adapter ViT-B/16 [51] IN21K 8 Ã—3Ã—1 455 33 93 7 62.8 -\nST-Adapter ViT-B/16 [51] CLIP 32Ã—3Ã—11955 267 100 14 69.5 92.6\nEVL ViT-L/14 [40] CLIP 32Ã—3Ã—19641 1858 479 175 66.7 -\nAIM ViT-B/16 IN21K 8 Ã—3Ã—1 624 202 100 14 62.0 -\nAIM ViT-L/14 [75] CLIP 32Ã—3Ã—111508 3725 354 50 70.6 92.7\nZeroI2V ViT-B/16 IN21K 8 Ã—3Ã—1 422 0 86 0 65.3 -\nZeroI2V ViT-B/16 CLIP 8 Ã—3Ã—1 422 0 86 0 67.7 90.8\nZeroI2V ViT-B/16 CLIP 16Ã—3Ã—1844 0 86 0 69.4 91.7\nZeroI2V ViT-B/16 CLIP 32Ã—3Ã—11688 0 86 0 70.1 92.4\nZeroI2V ViT-L/14 CLIP 8 Ã—3Ã—1 1946 0 304 0 70.1 91.8\nZeroI2V ViT-L/14 CLIP 16Ã—3Ã—13892 0 304 0 71.4 93.0\nZeroI2V ViT-L/14 CLIP 32Ã—3Ã—17783 0 304 0 72.2 93.0\n4.3 Fully-supervised Experiments\nResults on K400 As shown in Table 4, our method has significant advantages\nover traditional full fine-tuning methods, achieving better performance with much\nlower computational cost. For example, our ZeroI2V ViT-L/14 with an input of\n8 frames outperforms MViTv2 [36] (86.3 vs 86.1), while requiring more than 20\ntimes fewer GFLOPs (1946 vs 42420). Compared to multi-modal methods such\nas ActionCLIP [66] and X-CLIP [48], which require an additional text branch\nand fine-tune the entire model end-to-end, our ZeroI2V can achieve comparable\nperformance using only the visual encoder. Moreover, although our proposed\nZeroI2V doesnâ€™t increase computational or parameter costs during inference\ncompared with the previous PETL method, it can still achieve similar or even\nbetter performance. For example, on ViT-B/16, ZeroI2V with an input of 8\nframes can surpass ST-Adapter [51] with an input of 32 frames (83.0 vs 82.7)\nwith much lower GFLOPs (422 vs 1821). On ViT-L/14, ZeroI2V achieves the\nsame performance as EVL [40], which requires an additional 58M parameters.\nAnd ZeroI2V achieves comparable performance to AIM [75] (87.2 vs 87.5) with a\nnearly 30% reduction in GFLOPs (7783 vs 11208).\nResults on SSv2 As shown in Table 5, thanks to the effectiveness of STDHA\nin temporal modeling, our method outperforms most full fine-tuning methods,\neven though many of them have been pre-trained on the Kinetics dataset. Our\nZeroI2V has a significant improvement compared to directly full fine-tuning ViT-\nL/16 pre-trained with CLIP (70.1 vs 48.7) with the same number of parameters",
      "char_count": 2997
    },
    {
      "page_number": 12,
      "content": "12 X. Li et al.\nTable 6: Comparing the state-of-the-art video recognition methods on\nUCF101, HMDB51 and Diving48. For UCF101 and HMDB51, we test our method\nand report the 3-split mean Top-1 accuracy for both datasets following ST-Adapter [51].\nAnd for Diving48, we test our method with 1 temporal clip following AIM [75].\nMethod Pretrain UCF101 HMDB51 Diving48\nMethods with full fine-tuning\nI3D [8] ImageNet+K400 95.6 74.8 -\nS3D [71] ImageNet+K400 96.8 75.9 -\nSlowOnly-8x8-R101 [17] Kinetics+OmniSource 97.3 79.0 -\nVideoPrompt [26] CLIP 93.6 66.4 -\nTimeSformer-L [4] IN21K - - 81.0\nVideoSwin-B [44] IN21K - - 81.9\nMethods with PETL\nAIM ViT-B/16 [75] CLIP - - 88.9\nAIM ViT-L/14 [75] CLIP - - 90.6\nST-Adapter ViT-B/16 [51] CLIP+K400 96.4 77.7 -\nST-Adapter ViT-L/14 [51] CLIP+K400 98.1 81.7 -\nZeroI2V ViT-B/16 CLIP 95.6 73.7 89.7\nZeroI2V ViT-B/16 CLIP+K400 97.7 78.5 -\nZeroI2V ViT-L/14 CLIP 97.8 79.9 91.4\nZeroI2V ViT-L/14 CLIP+K400 98.6 83.4 -\nand computation. Compared to other PETL methods, ZeroI2V outperforms\nST-Adapter [51] on ViT-B/16 (70.1 vs 69.5) with lower GFLOPs (1688 vs 1955).\nAdditionally, ZeroI2V significantly surpasses both EVL [40] and AIM [75] (71.4\nvs 66.7, 70.6) on ViT-L/14 with much lower GFLOPs (3892 vs 9641, 11508) and\nnew parameters (0M vs 175M, 50M).\nResults on smaller datasets As shown in Table 6, on three relatively small\ndatasets, our method achieves state-of-the-art performance on UCF101, HMDB51,\nand Diving48. This demonstrates a clear performance advantage over both full-\nfinetuning methods and PETL methods previously.\nResults on action detection In addition to the task of action recognition, to\nunderstand the capability of our method in fine-grained spatial understanding,\nwe also evaluate our method on action detection dataset AVA [19]. Following the\nsetting of VideoMAE [60], we evaluate the top 60 common classes using the mean\nAverage Precision (mAP) as the metric under an IoU threshold of 0.5. As shown\nin Table 7, compared to using the original image CLIP features, our ZeroI2V\nachieved a significant performance improvement (26.4 vs 18.3) with the same\nnumber of parameters and computation. Itâ€™s noteworthy that our method was not\npre-trained on action recognition datasets such as Kinetics. Instead, we directly\napplied image-to-video transfer on the AVA dataset. Remarkably, our method\nstill managed to achieve performance on par with full-finetuning methods and\nself-supervised methods that underwent pre-training using the Kinetics dataset,\neven when using only 8 frames as input. In summary, our ZeroI2V demonstrates\noutstanding potential in video tasks beyond recognition.",
      "char_count": 2627
    },
    {
      "page_number": 13,
      "content": "ZeroI2V 13\nTable8:ComparingtheSoTAvideorecognitionmethodsontheVidTAB[34] .\nAction Science Safety Quality Emotion# Pretrain Data\nAverage\nDark Scene\nLong Video\nMedical Surgery\nAnimal Behavior\nHarmful Content\nFake Face\nQuality Assess\nEmotion Analysis\nCLIP-L [54] CLIP 42.831.2 38.0 32.3 36.3 50.3 58.5 67.7 28.1\nViCLIP-L [68] CLIP+InternVid200M 42.736.7 43.9 30.2 36.8 46.9 54.8 65.4 27.2\nST-Adapter-CLIP-L [51] CLIP 46.943.0 45.0 31.2 39.4 49.4 64.9 72.3 29.9\nZeroI2V-CLIP-L CLIP 46.541.3 46.8 31.2 39.3 47.2 64.6 70.6 30.6\nTable 9: Inference latency and throughput . All results are obtained using the\nsame V100-32G with PyTorch-builtin mixed precision, using a batch size of 1 to measure\nlatency and the optimal possible batch size to measure throughput before out of memory.\nModel Views GFLOPs Latency (ms) Throughput (V/s) K400 (Top-1) SSv2 (Top-1)\nUniformer-B [30] 32 Ã—4 1036 245.38 4.24 82.9 -\nEVL ViT-B/16 [40] 8 Ã—3 454 53.87 24.04 82.9 61.0\nViT-B/16 [13] 8 Ã—3 422 28.72 40.08 81.0 44.0\nZeroI2V ViT-B/16 8 Ã—3 422 28.89 40.08 83.0 67.7\n4.4 Few-shot Experiments\nTable 7: Comparing the state-\nof-the-art action detection\nmethods on AVA 2.2.\nMethod Pretrain Frames mAP\nSlowFast-R101 [17] K400 8 23.8\nMViTv2-B [36] K400 32 28.1\nVideoMAE-B [60] K400 16 31.8\nVideoMAE-B [60] K400 wo/ labels 16 26.7\nCLIP ViT-B/16 CLIP 8 18.3\nZeroI2V ViT-B/16 CLIP 8 26.4To demonstrate the adaptation capability of our\nmethod in few-shot scenarios, we conduct ex-\nperiments on the Video Task Adaptation Bench-\nmark (VidTAB). As show in Table 8 The results\nshow that our method can effectively enhance\nthe adaptation of the image model to video\ntasks using only a few samples. Compared to\nST-Adapter [51], our approach achieves compa-\nrable results while enjoying the advantage of parameter and inference efficiency.\n4.5 Efficiency analysis\nComparison of inference efficiency We compared the inference efficiency\nof our method with other methods on the same hardware device. As shown\nin Table 9, under comparable accuracy, the throughput of our method is 10\ntimes that of Uniformer [30], Compared to the original ViT-B, our method\nintroduces negligible additional latency during inference while achieving superior\nperformance. In comparison with EVL [40], it can also be seen that the impact\nof the additional computational module on the actual runtime latency (28.89 ms\nvs 53.87 ms) is greater than that reflected by GFLOPs (422 vs 454).\nComparison of training cost We compared the training cost of our method\nwith previous methods in Table 10. It can be seen that compared to previous full\nfine-tuning methods such as Uniformer [30] and ActionCLIP [66], our method\nsignificantly reduces training cost. Compared to the previous PETL method, our\nmethod does not have a significant advantage in training efficiency due to the",
      "char_count": 2803
    },
    {
      "page_number": 14,
      "content": "14 X. Li et al.\nTable 10: Comparison of training cost . Our results are obtained using a same\nV100-32G with PyTorch-builtin mixed precision, following EVL [40]. \" â€ \" indicates that\nthe epoch is estimated based on the batch size and training steps of the original paper.\n\"Memory\" refers to the GPU memory usage when the batch size is 8.\nModel (Frames) DatasetTraining\nEpochsTraining\nGPU HoursTunable Param (M) Memory (G) Top-1\nUniformer-B [30] (32) K400 110 5000 Ã—V100 50 - 82.9\nActionCLIP ViT-B/16 [66] (16) K400 50 480 Ã—RTX3090 142 - 82.6\nEVL ViT-B/16 [40] (8)K400 53 â€  60Ã—V100 29 2.2 82.9\nSSv2 46 â€  75Ã—V100 98 5.6 61.0\nST-Adapter ViT-B/16 [51] (8)K400 11 â€  23Ã—V100 7 6.9 82.0\nSSv2 38 â€  60Ã—V100 14 7.6 67.1\nAIM ViT-B/16 [75] (8)K400 30 120 Ã—V100 11 8.7 83.9\nSSv2 50 150 Ã—V100 14 9.0 66.4\nZeroI2V ViT-B/16 (8)K400 40 100 Ã—V100 14 7.6 83.0\nSSv2 50 90 Ã—V100 14 7.6 67.3\nuse of dense adapters. EVL [40], which does not need to insert adapters into\nthe frozen backbone, avoids some of the cost of backpropagation and therefore\nhas lower memory usage. ST-Adapter [51], due to its fewer trainable parameters,\nhas a faster convergence speed, but its memory usage is close to our method.\nNonetheless, in contrast to AIM [75] that imposes an additional computational\nburden for temporal modeling, our STDHA method, which does not introduce\nextra learnable parameters, ensures that ZeroI2V maintains superior training\nefficiency. We believe that it is worthwhile and acceptable to exchange some\ntraining costs for a reduction in inference costs. We will also try to further reduce\ntraining costs by improving training and adaptation strategies in the future.\n5 Conclusions\nIn this work, we present a new approach for parameter-efficient image-to-video\ntransfer learning , called ZeroI2V. By fully leveraging the powerful representational\ncapabilities of pre-trained image models, our approach enables image transformers\nto perform video tasks without introducing extra costs during inferences. Our\nproposed STDHA achieves efficient spatial-temporal modeling at zero extra\ncomputation and parameters. In addition, through structural reparameterization\nand full adaptation strategies, we successfully use a linear structure to achieve zero\nextra inference cost image-to-video adaptation for the first time. ZeroI2V shows\nstrong performance compared to previous full fine-tuning and PETL methods\non five widely used action recognition benchmarks while maintaining parameter\nand inference efficiency. Due to the simplicity and versatility of our method,\nwe believe it can be easily extended to other video tasks and even multi-modal\nunderstanding tasks. We will further investigate this direction in future work.\nAcknowledgements. ThisworkissupportedbytheNationalKeyR &DProgram\nof China (No. 2022ZD0160900), the National Natural Science Foundation of China\n(No. 62076119, No. 61921006), the Fundamental Research Funds for the Central\nUniversities (No. 020214380119), and the Collaborative Innovation Center of\nNovel Software Technology and Industrialization.",
      "char_count": 3044
    },
    {
      "page_number": 15,
      "content": "ZeroI2V 15\nReferences\n1.Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lucic, M., Schmid, C.: Vivit: A\nvideo vision transformer. In: Int. Conf. Comput. Vis. pp. 6816â€“6826 (2021)\n2.Ba, L.J., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint\narXiv:1607.06450 (2016)\n3.Bao, H., Dong, L., Piao, S., Wei, F.: Beit: BERT pre-training of image transformers.\nIn: Int. Conf. Learn. Represent. (2022)\n4.Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need for\nvideo understanding? In: Int. Conf. Mach. Learn. vol. 139, pp. 813â€“824 (2021)\n5.Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot\nlearners. In: Adv. Neural Inform. Process. Syst. vol. 33, pp. 1877â€“1901 (2020)\n6.Bulat, A., PÃ©rez-RÃºa, J., Sudhakaran, S., MartÃ­nez, B., Tzimiropoulos, G.: Space-\ntime mixing attention for video transformer. In: Adv. Neural Inform. Process. Syst.\npp. 19594â€“19607 (2021)\n7.Caron, M., Touvron, H., Misra, I., JÃ©gou, H., Mairal, J., Bojanowski, P., Joulin, A.:\nEmerging properties in self-supervised vision transformers. In: Int. Conf. Comput.\nVis. pp. 9630â€“9640 (2021)\n8.Carreira, J., Zisserman, A.: Quo vadis, action recognition? A new model and the\nkinetics dataset. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 4724â€“4733 (2017)\n9.Chen, S., Ge, C., Tong, Z., Wang, J., Song, Y., Wang, J., Luo, P.: Adaptformer:\nAdapting vision transformers for scalable visual recognition. In: Adv. Neural Inform.\nProcess. Syst. (2022)\n10.Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C.,\nSchuhmann, C., Schmidt, L., Jitsev, J.: Reproducible scaling laws for contrastive\nlanguage-image learning. arXiv preprint arXiv:2212.07143 (2022)\n11.Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.: Randaugment: Practical automated data\naugmentation with a reduced search space. In: Adv. Neural Inform. Process. Syst.\n(2020)\n12.Devlin, J., Chang, M., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In: Proceedings of NAACL-HLT.\npp. 4171â€“4186 (2019)\n13.Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\nAn image is worth 16x16 words: Transformers for image recognition at scale. In:\nInt. Conf. Learn. Represent. (2021)\n14.Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., Feichtenhofer, C.:\nMultiscale vision transformers. In: Int. Conf. Comput. Vis. pp. 6804â€“6815 (2021)\n15.Fan, Q., Chen, C.F., Panda, R.: Can an image classifier suffice for action recognition?\nIn: Int. Conf. Learn. Represent. (2021)\n16.Feichtenhofer, C.: X3D: expanding architectures for efficient video recognition. In:\nIEEE Conf. Comput. Vis. Pattern Recog. pp. 200â€“210 (2020)\n17.Feichtenhofer, C., Fan, H., Malik, J., He, K.: Slowfast networks for video recognition.\nIn: Int. Conf. Comput. Vis. pp. 6201â€“6210 (2019)\n18.Goyal, R., Kahou, S.E., Michalski, V., Materzynska, J., Westphal, S., Kim, H.,\nHaenel, V., FrÃ¼nd, I., Yianilos, P., Mueller-Freitag, M., Hoppe, F., Thurau, C.,\nBax, I., Memisevic, R.: The \"something something\" video database for learning\nand evaluating visual common sense. In: Int. Conf. Comput. Vis. pp. 5843â€“5851.\nIEEE Computer Society (2017)",
      "char_count": 3354
    },
    {
      "page_number": 16,
      "content": "16 X. Li et al.\n19.Gu, C., Sun, C., Ross, D.A., Vondrick, C., Pantofaru, C., Li, Y., Vijayanarasimhan,\nS., Toderici, G., Ricco, S., Sukthankar, R., et al.: Ava: A video dataset of spatio-\ntemporally localized atomic visual actions. In: IEEE Conf. Comput. Vis. Pattern\nRecog. pp. 6047â€“6056 (2018)\n20. He, K., Chen, X., Xie, S., Li, Y., DollÃ¡r, P., Girshick, R.B.: Masked autoencoders\nare scalable vision learners. In: IEEE Conf. Comput. Vis. Pattern Recog. pp.\n15979â€“15988 (2022)\n21.He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.B.: Momentum contrast for unsuper-\nvised visual representation learning. In: IEEE Conf. Comput. Vis. Pattern Recog.\npp. 9726â€“9735 (2020)\n22.He,X.,Li,C.,Zhang,P.,Yang,J.,Wang,X.E.:Parameter-efficientmodeladaptation\nfor vision transformers. arXiv preprint arXiv:2203.16329 (2022)\n23.Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., de Laroussilhe, Q., Gesmundo,\nA., Attariyan, M., Gelly, S.: Parameter-efficient transfer learning for NLP. In: Int.\nConf. Mach. Learn. vol. 97, pp. 2790â€“2799 (2019)\n24.Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen,\nW.: Lora: Low-rank adaptation of large language models. In: Int. Conf. Learn.\nRepresent. (2022)\n25.Jia, M., Tang, L., Chen, B.C., Cardie, C., Belongie, S., Hariharan, B., Lim, S.N.:\nVisual prompt tuning. In: Eur. Conf. Comput. Vis. pp. 709â€“727 (2022)\n26.Ju, C., Han, T., Zheng, K., Zhang, Y., Xie, W.: Prompting visual-language models\nfor efficient video understanding. In: Eur. Conf. Comput. Vis. pp. 105â€“124. Springer\n(2022)\n27.Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., Serre, T.: Hmdb: a large video\ndatabase for human motion recognition. In: Int. Conf. Comput. Vis. pp. 2556â€“2563.\nIEEE (2011)\n28.Lester, B., Al-Rfou, R., Constant, N.: The power of scale for parameter-efficient\nprompt tuning. In: Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing. pp. 3045â€“3059 (2021)\n29.Li, J., Li, D., Xiong, C., Hoi, S.C.H.: BLIP: bootstrapping language-image pre-\ntraining for unified vision-language understanding and generation. In: Int. Conf.\nMach. Learn. vol. 162, pp. 12888â€“12900 (2022)\n30.Li, K., Wang, Y., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified\ntransformer for efficient spatial-temporal representation learning. In: Int. Conf.\nLearn. Represent. (2022)\n31.Li, K., Wang, Y., He, Y., Li, Y., Wang, Y., Wang, L., Qiao, Y.: Uniformerv2:\nUnlocking the potential of image vits for video understanding. In: Int. Conf. Comput.\nVis. pp. 1632â€“1643 (2023)\n32.Li, T., Wang, L.: Learning spatiotemporal features via video and text pair discrimi-\nnation. arXiv preprint arXiv:2001.05691 (2020)\n33.Li, X.L., Liang, P.: Prefix-tuning: Optimizing continuous prompts for generation.\nIn: Proceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers). pp. 4582â€“4597 (2021)\n34.Li, X., Huang, Z., Wang, J., Li, K., Wang, L.: Videoeval: Comprehensive bench-\nmark suite for low-cost evaluation of video foundation model. arXiv preprint\narXiv:2407.06491 (2024)\n35.Li, Y., Ji, B., Shi, X., Zhang, J., Kang, B., Wang, L.: TEA: temporal excitation and\naggregation for action recognition. In: IEEE Conf. Comput. Vis. Pattern Recog.\npp. 906â€“915 (2020)",
      "char_count": 3329
    },
    {
      "page_number": 17,
      "content": "ZeroI2V 17\n36.Li, Y., Wu, C., Fan, H., Mangalam, K., Xiong, B., Malik, J., Feichtenhofer, C.:\nMvitv2: Improved multiscale vision transformers for classification and detection.\nIn: IEEE Conf. Comput. Vis. Pattern Recog. pp. 4794â€“4804 (2022)\n37.Li, Y., Li, Y., Vasconcelos, N.: Resound: Towards action recognition without\nrepresentation bias. In: Eur. Conf. Comput. Vis. pp. 513â€“528 (2018)\n38.Lian, D., Zhou, D., Feng, J., Wang, X.: Scaling & shifting your features: A new\nbaseline for efficient model tuning. In: Adv. Neural Inform. Process. Syst. (2022)\n39.Lin, J., Gan, C., Wang, K., Han, S.: TSM: temporal shift module for efficient and\nscalable video understanding on edge devices. IEEE Trans. Pattern Anal. Mach.\nIntell.44(5), 2760â€“2774 (2022)\n40.Lin, Z., Geng, S., Zhang, R., Gao, P., de Melo, G., Wang, X., Dai, J., Qiao, Y., Li,\nH.: Frozen CLIP models are efficient video learners. In: Eur. Conf. Comput. Vis.\nvol. 13695, pp. 388â€“404 (2022)\n41.Liu, M., Wang, Z., Ji, S.: Non-local graph neural networks. IEEE Trans. Pattern\nAnal. Mach. Intell. 44(12), 10270â€“10276 (2022)\n42.Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z.,\nDong, L., Wei, F., Guo, B.: Swin transformer V2: scaling up capacity and resolution.\nIn: IEEE Conf. Comput. Vis. Pattern Recog. pp. 11999â€“12009 (2022)\n43.Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\ntransformer: Hierarchical vision transformer using shifted windows. In: Int. Conf.\nComput. Vis. pp. 9992â€“10002 (2021)\n44.Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., Hu, H.: Video swin transformer.\nIn: IEEE Conf. Comput. Vis. Pattern Recog. pp. 3192â€“3201 (2022)\n45.Liu, Z., Wang, L., Wu, W., Qian, C., Lu, T.: TAM: temporal adaptive module for\nvideo recognition. In: Int. Conf. Comput. Vis. pp. 13688â€“13698 (2021)\n46.Lu, C., Jin, X., Huang, Z., Hou, Q., Cheng, M., Feng, J.: CMAE-V: contrastive\nmasked autoencoders for video action recognition. arXiv preprint arXiv:2301.06018\n(2023)\n47.Michel, P., Levy, O., Neubig, G.: Are sixteen heads really better than one? In: Adv.\nNeural Inform. Process. Syst. pp. 14014â€“14024 (2019)\n48.Ni, B., Peng, H., Chen, M., Zhang, S., Meng, G., Fu, J., Xiang, S., Ling, H.:\nExpanding language-image pretrained models for general video recognition. In: Eur.\nConf. Comput. Vis. vol. 13664, pp. 1â€“18 (2022)\n49.Nie, X., Ni, B., Chang, J., Meng, G., Huo, C., Zhang, Z., Xiang, S., Tian, Q., Pan, C.:\nPro-tuning: Unified prompt tuning for vision tasks. arXiv preprint arXiv:2207.14381\n(2022)\n50.Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V.,\nFernandez, P., Haziza, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba,\nW., Howes, R., Huang, P., Li, S., Misra, I., Rabbat, M.G., Sharma, V., Synnaeve,\nG., Xu, H., JÃ©gou, H., Mairal, J., Labatut, P., Joulin, A., Bojanowski, P.: Dinov2:\nLearningrobustvisualfeatureswithoutsupervision.arXivpreprintarXiv:2304.07193\n(2023)\n51.Pan, J., Lin, Z., Zhu, X., Shao, J., Li, H.: St-adapter: Parameter-efficient image-to-\nvideo transfer learning. In: Adv. Neural Inform. Process. Syst. (2022)\n52.Pfeiffer, J., Kamath, A., RÃ¼cklÃ©, A., Cho, K., Gurevych, I.: Adapterfusion: Non-\ndestructive task composition for transfer learning. In: Proceedings of the 16th\nConference of the European Chapter of the Association for Computational Linguis-\ntics: Main Volume. pp. 487â€“503 (2021)\n53.Pfeiffer,J.,RÃ¼cklÃ©,A.,Poth,C.,Kamath,A.,VuliÄ‡,I.,Ruder,S.,Cho,K.,Gurevych,\nI.: Adapterhub: A framework for adapting transformers. In: Proceedings of the",
      "char_count": 3538
    },
    {
      "page_number": 18,
      "content": "18 X. Li et al.\n2020 Conference on Empirical Methods in Natural Language Processing: System\nDemonstrations. pp. 46â€“54 (2020)\n54.Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,\nAskell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable\nvisualmodelsfromnaturallanguagesupervision.In:Int.Conf.Mach.Learn.vol.139,\npp. 8748â€“8763 (2021)\n55.Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving language\nunderstanding by generative pre-training. OpenAI blog (2018)\n56.Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language\nmodels are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019)\n57.Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-\ncam: Visual explanations from deep networks via gradient-based localization. Int.\nJ. Comput. Vis. 128(2), 336â€“359 (2020)\n58.Soomro, K., Zamir, A.R., Shah, M.: Ucf101: A dataset of 101 human actions classes\nfrom videos in the wild. arXiv preprint arXiv:1212.0402 (2012)\n59.Tan, J., Zhao, X., Shi, X., Kang, B., Wang, L.: Pointtad: Multi-label temporal\naction detection with learnable query points. NIPS 35, 15268â€“15280 (2022)\n60.Tong, Z., Song, Y., Wang, J., Wang, L.: Videomae: Masked autoencoders are\ndata-efficient learners for self-supervised video pre-training. In: Adv. Neural Inform.\nProcess. Syst. (2022)\n61.Tschannen, M., Mustafa, B., Houlsby, N.: Clippo: Image-and-language understand-\ning from pixels only. arXiv preprint arXiv:2212.08045 (2022)\n62.Tu, S., Dai, Q., Wu, Z., Cheng, Z., Hu, H., Jiang, Y.: Implicit temporal modeling\nwith learnable alignment for video recognition. In: Int. Conf. Comput. Vis. (2023)\n63.Wang, L., Huang, B., Zhao, Z., Tong, Z., He, Y., Wang, Y., Wang, Y., Qiao, Y.:\nVideomae V2: scaling video masked autoencoders with dual masking. In: IEEE\nConf. Comput. Vis. Pattern Recog. (2023)\n64.Wang, L., Tong, Z., Ji, B., Wu, G.: TDN: temporal difference networks for efficient\naction recognition. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 1895â€“1904\n(2021)\n65.Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Gool, L.V.: Temporal\nsegment networks: Towards good practices for deep action recognition. In: Eur.\nConf. Comput. Vis. vol. 9912, pp. 20â€“36 (2016)\n66.Wang, M., Xing, J., Liu, Y.: Actionclip: A new paradigm for video action recognition.\narXiv preprint arXiv:2109.08472 (2021)\n67.Wang, R., Chen, D., Wu, Z., Chen, Y., Dai, X., Liu, M., Jiang, Y., Zhou, L., Yuan,\nL.: BEVT: BERT pretraining of video transformers. In: IEEE Conf. Comput. Vis.\nPattern Recog. pp. 14713â€“14723 (2022)\n68.Wang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Li, X., Chen, G., Chen, X., Wang,\nY., et al.: Internvid: A large-scale video-text dataset for multimodal understanding\nand generation. In: ICLR (2024)\n69.Wu, W., Sun, Z., Ouyang, W.: Revisiting classifier: Transferring vision-language\nmodels for video recognition. In: AAAI Conf. Artif. Intell. pp. 2847â€“2855 (2023)\n70.Xiang, W., Li, C., Wang, B., Wei, X., Hua, X., Zhang, L.: Spatiotemporal self-\nattention modeling with temporal patch shift for action recognition. In: Eur. Conf.\nComput. Vis. vol. 13663, pp. 627â€“644 (2022)\n71.Xie, S., Sun, C., Huang, J., Tu, Z., Murphy, K.: Rethinking spatiotemporal feature\nlearning: Speed-accuracy trade-offs in video classification. In: Eur. Conf. Comput.\nVis. pp. 305â€“321 (2018)",
      "char_count": 3393
    },
    {
      "page_number": 19,
      "content": "ZeroI2V 19\n72.Xu, C., Zhu, Y., Shen, H., , Chen, B., Liao, Y., Chen, X., Wang, L.: Progres-\nsive visual prompt learning with contrastive feature re-formation. arXiv preprint\narXiv:2304.08386 (2023)\n73.Xu, C., Zhu, Y., Zhang, G., Shen, H., Liao, Y., Chen, X., Wu, G., Wang,\nL.: Dpl: Decoupled prompt learning for vision-language models. arXiv preprint\narXiv:2308.10061 (2023)\n74.Yan, S., Xiong, X., Arnab, A., Lu, Z., Zhang, M., Sun, C., Schmid, C.: Multiview\ntransformers for video recognition. In: IEEE Conf. Comput. Vis. Pattern Recog.\npp. 3323â€“3333 (2022)\n75.Yang, T., Zhu, Y., Xie, Y., Zhang, A., Chen, C., Li, M.: Aim: Adapting image\nmodels for efficient video action recognition. In: Int. Conf. Learn. Represent. (2023)\n76.Zaken, E.B., Goldberg, Y., Ravfogel, S.: Bitfit: Simple parameter-efficient fine-\ntuning for transformer-based masked language-models. In: Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers). pp. 1â€“9 (2022)\n77.Zhai, X., Kolesnikov, A., Houlsby, N., Beyer, L.: Scaling vision transformers. In:\nIEEE Conf. Comput. Vis. Pattern Recog. pp. 1204â€“1213 (2022)\n78.Zhang, G., Zhu, Y., Wang, H., Chen, Y., Wu, G., Wang, L.: Extracting motion\nand appearance via inter-frame attention for efficient video frame interpolation. In:\nIEEE Conf. Comput. Vis. Pattern Recog. (2023)\n79.Zhang, H., Hao, Y., Ngo, C.: Token shift transformer for video classification. In:\nACM Int. Conf. Multimedia. pp. 917â€“925 (2021)\n80.Zhang, Y., Zhou, K., Liu, Z.: Neural prompt search. arXiv preprint arXiv:2206.04673\n(2022)\n81.Zhong, Z., Zheng, L., Kang, G., Li, S., Yang, Y.: Random erasing data augmentation.\nIn: AAAI Conf. Artif. Intell. pp. 13001â€“13008 (2020)\n82. Zhou, B., Andonian, A., Oliva, A., Torralba, A.: Temporal relational reasoning in\nvideos. In: Eur. Conf. Comput. Vis. vol. 11205, pp. 831â€“846 (2018)\n83.Zhu,Y.,Ji,Y.,Zhao,Z.,Wu,G.,Wang,L.:Awt:Transferringvision-languagemodels\nvia augmentation, weighting, and transportation. arXiv preprint arXiv:2407.04603\n(2024)\n84.Zhu, Y., Zhang, G., Tan, J., Wu, G., Wang, L.: Dual detrs for multi-label temporal\naction detection. In: CVPR. pp. 18559â€“18569 (2024)",
      "char_count": 2173
    },
    {
      "page_number": 20,
      "content": "20 X. Li et al.\n6 Appendix\nIn this appendix, we provide more details of ZeroI2V from the following aspects:\nâ€“Implementation details of our method are in Â§ 6.1.\nâ€“Experimental results with other pre-trained weights and backbone architec-\ntures can be found in Â§ 6.2.\nâ€“Visualization of our proposed Spatial-Temporal Dual-Headed Attention\n(STDHA) is in Â§ 6.3.\nâ€“Limitations and societal impact are in Â§ 6.4\nâ€“License of the datasets and pre-trained models are in Â§ 6.5\n6.1 Implementation details of our method\nModel Details Our model details are shown in the Table 11a and Table 11b.\nDue to different requirements for spatial modeling and temporal modeling in\ndifferent datasets, there are slight differences in the specific implementation\nsettings.\n1.Settings of STDHA For the settings of STDHA, we allocate 1/6 to 1/4\nof the heads for temporal modeling based on the conclusions obtained from\nprevious ablation experiments. For long inputs, we increase the absolute value\nofâˆ†tto obtain a larger temporal receptive field. When using Swin-B as the\nbackbone, due to its four stages and different numbers of heads in each stage,\nwe simply allocate temporal heads to each stage at a ratio of 1/4. Since its\ninput length is halved after patch embedding (from 32 frames to 16 frames),\nwe set the value of âˆ†taccording to the best temporal receptive field of 16\nframes, which is 5. Please note that we have not tried other configurations\ndue to time constraints, so there may be better configurations.\n2.Number of adapters Considering the balance between performance and\ntraining cost, we only assign different adapters for each weight for the mini-\nmum setting (ViT-B with 8-frame input) on the SSv2 dataset, which requires\n6 adapters. For all other settings, we only use 4 adapters. And the bottleneck\nratios of all adapters are set to 0.25.\nTraining Details As shown in Table 12, our training strategy is similar to\nthe previous methods [51,75]. Considering that SSv2 requires stronger temporal\nmodeling ability, we used a stronger data augmentation strategy following [44].\nIn addition, for the full finetuing experiment using Swin-B as the backbone\n(Swin-B with STDHA), we use exactly the same training strategy as video swin\ntransformer [44].\nImplementationdetailsofadaptationstrategies Thetrainingconfigurations\nused for all the adaptation strategies are summarized as follows:\nâ€“For the comparison experiment of full finetuning ViT with CLIP pretrained,\nwe use 1/10 of the learning rate to avoid training collapse.",
      "char_count": 2507
    },
    {
      "page_number": 21,
      "content": "ZeroI2V 21\nTable 11: Model details. We use a multiset to represent the time offsets of different\nheads (e.g., \"1Â·2\" means that there are 2 heads with âˆ†t= 1). When âˆ†t= 0, it\nrepresents a spatial head.\"Temporal RF\" refers to temporal receptive field of a single\nSTDHA. \"Num. Adapters\" refers to the number of linear adapters per ViT block.\nBackbone Frames âˆ†tof headsTemporal\nRFNum.\nAdapters\nViT-B\n(h=12)8 {1Â·1,âˆ’1Â·1,0Â·10} 3 4\n16 {1Â·1,âˆ’1Â·1,2Â·1,0Â·9} 4 4\n32 {1Â·1,âˆ’1Â·1,2Â·1,âˆ’2Â·1,3Â·1,0Â·7} 6 4\nViT-L\n(h=16)8 {1Â·2,âˆ’1Â·2,0Â·12} 3 4\n16 {1Â·2,âˆ’1Â·2,2Â·1,0Â·11} 4 4\n32 {1Â·2,âˆ’1Â·2,2Â·1,âˆ’2Â·1,3Â·1,0Â·9} 6 4\n(a)Model details for Kinetics400.\nBackbone Frames âˆ†tof headsTemporal\nRFNum.\nAdapters\nViT-B\n(h=12)8 {1Â·1,âˆ’1Â·1,0Â·10} 3 6\n16 {1Â·1,âˆ’1Â·1,2Â·1,âˆ’2Â·1,0Â·8} 5 4\n32 {1Â·1,âˆ’1Â·1,2Â·1,âˆ’2Â·1,3Â·1,0Â·7} 6 4\nViT-L\n(h=16)8 {1Â·2,âˆ’1Â·2,0Â·12} 3 4\n16 {1Â·2,âˆ’1Â·2,2Â·2,âˆ’2Â·2,0Â·8} 5 4\n32 {1Â·2,âˆ’1Â·2,2Â·1,âˆ’2Â·1,3Â·1,0Â·9} 6 4\nSwin-B\n(h= 4,\n8, 16\n,32)32Stage 1: {1Â·1,0Â·3} 2\n4Stage 2: {1Â·1,âˆ’1Â·1,0Â·6} 3\nStage 3: {1Â·1,âˆ’1Â·1,2Â·1,âˆ’2Â·1,0Â·12}5\nStage 4: {1Â·2,âˆ’1Â·2,2Â·2,âˆ’2Â·2,0Â·24}5\n(b)Model details for Something-Something v2.\nâ€“For the comparison experiment of only tuning temporal head, we froze the\nparameters related to the spatial head (only training the part of the pa-\nrameters related to the temporal head, in other words, we only trained\nWQt\nattn, WKt\nattn, WVt\nattnâˆˆRdÃ—dt, WOt\nattnâˆˆRdtÃ—d, where dtis the number of chan-\nnels of the temporal head).\nFor any settings not explicitly mentioned, we assume they align with the\ntraining settings of the Linear Adapter.\n6.2 Additional experimental results\nExperiments with ImageNet21K pre-trained weights In order to investi-\ngate the adaptability of our method to different pre-trained weights, we conducted",
      "char_count": 1686
    },
    {
      "page_number": 22,
      "content": "22 X. Li et al.\nTable 12: Training details of our method.\ndataset K400 SSv2\nOptimization settings\noptimizer AdamW, learning rate=3e-4, weight decay=5e-2\nbatch size 64\ntraining epochs 40 50\nSampling settings\ncrop size 224\nframe sampling rate16 (for 8-frame input)\n8 (for 16-frame input)\n4 (for 32-frame input)uniformly sample as TSN [65]\nnum. testing views 3 temporal Ã—1 spatial 1 temporal Ã—3 spatial\nData augmentation settings\nRandAugment [11] m=7, n=4\nflip 0.5\nRandom erasing [81] - 0.25\nlabel smoothing - 0.1\nexperiments using the same model and training settings on ImageNet21K pre-\ntrained weights. The results are shown in Table 13. It can be seen that our method\nis still very effective under ImageNet21K weights and can surpass previous full\nfine-tuning methods. Compared to other PETL methods, our method shows\nstronger robustness. As shown in Figure 1, when using ImageNet21K pre-trained\nweights, the advantage of our method over other PETL methods is even greater\nthan when using CLIP pre-trained weights. For example, when using CLIP\nweights, our method slightly surpasses ST-Adapter [51] (67.7 vs 67.1), while when\nusing ImageNet21K weights, we have a clear advantage (65.3 vs 62.8).\nExperiments with other backbone In order to verify the universality of our\nmethod, we conducted experiments using the Swin Transformer [43] in Table\n14, which has a hierarchical structure and local window attention. As shown\nin Table 1, although our method is not specifically designed and adjusted for\nit, it can still achieve performance comparable or even better than other full\nfine-tuning methods. To our surprise, when we used a full fine-tuning strategy to\ntrain Swin-B using STDHA, we achieved a top-1 accuracy of 70%, which even\nsurpassed VideoSwin-B [44] pre-trained on the K400 dataset. From this, we can\nsee that our designed STHDA is not only versatile but also has powerful temporal\nmodeling capabilities. In addition, for backbones like Swin Transformer that have\nmore inductive bias and have not been trained on large-scale image-text datasets,\nfull fine-tuning may be able to better utilize the temporal modeling capabilities\nof STDHA.",
      "char_count": 2148
    },
    {
      "page_number": 23,
      "content": "ZeroI2V 23\nTable 13: Results on K400 and SSv2 validation set with ImageNet21K\npretrained. Views = #frames Ã—#spatial crops Ã—#temporal clips. â€œGFLOPsâ€ means\n109FLOPs, \"M\" means 106. â€œExtra GLOPsâ€ refers to the extra computation added to\nthe original ViT under the same number of views. \"New Params\" refers to additional\nparameters during inference besides the parameters of the original ViT backbone and\nlinear classifier. Views for all methods are 8 Ã—1Ã—3 for K400 and 8 Ã—3Ã—1 for SSv2\nMethods Pretrain GFLOPsExtra\nGFLOPsParam(M)New\nParam(M)K400\nTop-1SSv2\nTop-1\nMethods with full fine-tuning\nTimeSformer [4] IN21K 590 - 121 - 78.0 59.5\nX-ViT [6] IN21K 425 - 92 - 78.5 64.4\nMethods with PETL & ViT-B/16\nEVL [40] IN21K 454 32 115 29 75.4 -\nST-Adapter [51] IN21K 455 33 93 7 76.6 62.8\nAIM [75] IN21K 624 202 100 14 78.8 62.0\nZeroI2V IN21K 422 0 86 0 78.6 65.3\nTable 14: Results on SSv2 validation set with Swin-B backbone. K400 â€ indicates\nthat the model is pre-trained on both IN21K and K400. The other notations are the\nsame as Table 13\n.\nMethods Pretrain Views GFLOPs Param(M)Tunable\nParam(M)Top-1 Top-5\nVideoSwin-B [44] K400 â€ 32Ã—3Ã—1963 89 89 69.6 92.7\nPST-B [70] IN21K 32Ã—3Ã—1741 89 89 67.4 90.9\nSIFAR-B [15] IN21K 32Ã—3Ã—1789 87 87 62.6 88.5\nSwin-B w/ STDHA IN21K 32Ã—3Ã—1741 89 89 70.0 92.1\nZeroI2V Swin-B IN21K 32Ã—3Ã—1741 89 14 67.8 91.4\n6.3 Visualization\nThe motivation behind the design of STDHA is to enable simultaneous spatial\nand temporal modeling in an independent way before information fusion (ie.\ndecoupling spatial and temporal modeling). In order to more intuitively demon-\nstrate the temporal modeling capabilities of our proposed STDHA, we visualized\nthe attention map of the last layer of the network. As shown in Figure 3, note that\nwe visualize the attention map of the last transformer layer in our figure. Due to\nthe temporal receptive field increases with the network depth, both the spatial\nhead and the temporal head of this last layer have a global temporal receptive\nfield about the input frames. But we can still observe that the spatial heads\npay more attention to the information of the current frame while the temporal\nhead pays more attention to the information of other frames. We compare the\nattention maps of STDHA and CLIP, and it can be seen that STDHA pays more\nattention to the interaction of objects in the video (such as the touch between",
      "char_count": 2370
    },
    {
      "page_number": 24,
      "content": "24 X. Li et al.\nhands and cups or Rubikâ€™s cubes), while CLIP only focuses on individual objects\nand does not capture the spatio-temporal dynamics in the video well.\n6.4 Limitations and Societal Impact\nLimitations Our method has the following two main limitations:\nâ€“Although our method is very efficient during inference, the densely inserted\nlinear adapters still need to participate in gradient calculation during training,\nwhich brings a non-negligible training cost. This makes our method still have\na certain disadvantage in training cost compared to methods that use CLIP\nas an independent feature extractor (such as EVL [40]). In the future, we\nneed to consider more efficient training strategies and improve the structure\nof linear adapters to address this issue.\nâ€“Although STDHA has demonstrated powerful temporal modeling capabilities,\nit still requires consideration of the original number of heads in ViT and\nmanual design of a head relocation strategy. Despite the ablation experiment\nresults showing that our methodâ€™s performance is relatively stable across\ndifferent head relocation strategies, achieving better results still necessitates\nsome manual design. Obtaining optimal head relocation strategies through\nmanual design is obviously challenging. In future work, we aim to investigate\nmethods for automatically designing head relocation strategies.\nSocietal impact Our ZeroI2V method can apply existing image pre-trained\ntransformers as powerful backbone networks for video tasks such as video clas-\nsification, spatiotemporal action detection, and video segmentation. Although\nwe do not provide direct applications, it still has the potential to be applied to\nmany scenarios related to video tasks. On the positive side, a powerful video\nunderstanding backbone network can improve the performance of downstream\ntasks and thus enhance efficiency in various scenarios, such as in the fields of\nsmart healthcare and intelligent transportation where video understanding is\nrequired. On the other hand, if applied improperly, advanced video networks may\nalso have negative impacts, such as being used in terrorist military activities.\nResearchers need to carefully consider the potential risks and impacts when\napplying it to real-world scenarios.\n6.5 License of datasets and pre-trained models\nAll the datasets we used are commonly used datasets for academic purpose. The\nlicense of the Kinetics-4003is CC BY-NC 4.04. The license of the Something-\nSomething V25is custom. We used the publicly available CLIP pre-trained\nweights provided by OpenAI6and the Swin Transformer pre-trained weights\nprovided by Microsoft7, both of which use the MIT License.\n3https://www.deepmind.com/open-source/kinetics\n4https://creativecommons.org/licenses/by/4.0\n5https://developer.qualcomm.com/software/ai-datasets/something-something\n6https://github.com/openai/CLIP\n7https://github.com/microsoft/Swin-Transformer",
      "char_count": 2910
    },
    {
      "page_number": 25,
      "content": "ZeroI2V 25\nGround truth: Throwing something in the air and catching it Ground truth: Tearing something into two pieces\nGround truth: Pulling something from left to right Ground truth: Spinning something that quickly stops spinningAttention maps of CLIP Attention maps of CLIP\nAttention maps of Spatial heads\nAttention maps of Spatial headsAttention maps of Spatial heads\nAttention maps of Temporal heads Attention maps of Temporal heads\nAttention maps of Temporal headsAttention maps of STDHA\nAttention maps of Temporal headsAttention maps of Spatial headsAttention maps of CLIP Attention maps of CLIPAttention maps of STDHA\nAttention maps of STDHA Attention maps of STDHA\nFig.3: Visualization of attention maps of CLIP, spatial heads, temporal\nheads and STDHA at the last layer generated by Grad-CAM [57] on SSv2\nvalidation set.",
      "char_count": 829
    }
  ],
  "total_chars": 66913
}