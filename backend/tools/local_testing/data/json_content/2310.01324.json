(aws-hack) D:\Projects\AWS Hackathon\scientific-discovery-agent>python backend\tools\local_testing\local_agent.py

>>>> Turn 1 <<<<
...Agent is thinking...
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1758260506.799171   21812 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.
...Agent decided to use external tool: 'search_arxiv' with input: {'query': 'Tree of Thoughts'}
✅ Tool 'search_arxiv' Executed Successfully. Result:
[
  {
    "title": "From Linear to Hierarchical: Evolving Tree-structured Thoughts for\n  Efficient Alpha Mining",
    "authors": [
      "Junji Ren",
      "Junjie Zhao",
      "Shengcai Liu",
      "Peng Yang"
    ],
    "abstract": "Alpha mining, which discovers signals that predict asset returns, has long\nbeen attractive for automatic quantitative investment. This problem is\ntypically formulated as a tree-based symbolic regression with handcrafted\nmarket data features and arithmetic operators. Unfortunately, existing symbolic\nmethods are concerned with computational inefficiency and dependence on prior\nknowledge. Recent implementation of Large Language Models (LLMs) show that they\ncan automatically generate executable codes for various tasks efficiently, thus\ncan be considered as a new promising way for alpha mining. Specifically,\nLLMs-driven methods evolve a set of heuristics, including thoughts and codes,\nwhere the thoughts are usually represented as plain-text prompts of codes.\nUnfortunately, trivially adopting them in alpha mining ignores the fact that\nalphas are with hierarchical tree structures. This paper introduces\nTree-structured thought Evolution (TreEvo), which evolves hierarchical\nreasoning ideas solely at the thought level. Experiments on four real-market\ndatasets demonstrate that TreEvo can obtain better alphas with much less\ncomputational time and human expert efforts. And this superiority hardly holds\nwithout the tree-structured thoughts and the compatible evolutionary operators.",
    "url": "http://arxiv.org/abs/2508.16334v1"
  }
]

>>>> Turn 2 <<<<
...Agent is thinking...
...Agent decided to use internal utility: 'convert_to_pdf_url' with input: {'url': 'http://arxiv.org/abs/2508.16334v1'}
✅ Internal Utility 'convert_to_pdf_url' Executed Successfully. Result:
http://arxiv.org/pdf/2508.16334v1.pdf

>>>> Turn 3 <<<<
...Agent is thinking...
...Agent decided to use external tool: 'extract_pdf_text' with input: {'pdf_urls': ['http://arxiv.org/pdf/2508.16334v1.pdf']}
✅ Tool 'extract_pdf_text' Executed Successfully. Result:
[
  {
    "paper_id": "2508.16334v1",
    "source": "arxiv",
    "pdf_url": "http://arxiv.org/pdf/2508.16334v1.pdf",
    "abstract_url": "http://arxiv.org/abs/2508.16334v1",
    "extracted_timestamp": 1758278510,
    "status": "success",
    "total_pages": 9,
    "pages": [
      {
        "page_number": 1,
        "content": "From Linear to Hierarchical: Evolving Tree-structured Thoughts\nfor Efficient Alpha Mining\nJunji Ren1, Junjie Zhao2, Shengcai Liu1, Peng Yang1\nAbstract\nAlpha mining, which discovers signals that predict asset\nreturns, has long been attractive for automatic quanti-\ntative investment. This problem is typically formulated\nas a tree-based symbolic regression with handcrafted\nmarket data features and arithmetic operators. Unfortu-\nnately, existing symbolic methods are concerned with\ncomputational inefficiency and dependence on prior\nknowledge. Recent implementation of Large Language\nModels (LLMs) show that they can automatically gen-\nerate executable codes for various tasks efficiently, thus\ncan be considered as a new promising way for alpha\nmining. Specifically, LLMs-driven methods evolve a set\nof heuristics, including thoughts and codes, where the\nthoughts are usually represented as plain-text prompts\nof codes. Unfortunately, trivially adopting them in alpha\nmining ignores the fact that alphas are with hierarchical\ntree structures. This paper introduces Tree-structured\nthought Evolution (TreEvo), which evolves hierarchi-\ncal reasoning ideas solely at the thought level. Exper-\niments on four real-market datasets demonstrate that\nTreEvo can obtain better alphas with much less compu-\ntational time and human expert efforts. And this supe-\nriority hardly holds without the tree-structured thoughts\nand the compatible evolutionary operators.\nIntroduction\nAlpha mining, an important endeavor within financial quan-\ntitative trading, is to rigorously identify measurable sig-\nnals that predict asset returns. The core challenge of al-\npha mining lies in isolating truly predictive factors from\nnoisy, non-stationary data efficiently while ensuring inter-\npretability (Giglio, Kelly, and Xiu 2022; Belle and Papanto-\nnis 2021).\nWith the rise of the sub-symbolic paradigm in recent\ndecades, neural networks have been increasingly applied to\nalpha mining (Abe and Nakayama 2018; Chen, Pelger, and\nZhu 2024). These methods effectively address challenges\non automatic feature extraction and complex nonlinear re-\nlationships modeling (Jiang 2021). However, taking neural\nnetwork as alpha itself is often criticized for limited inter-\npretability, whose \u201cblack-box\u201d nature restricts widespread\n1Southern University of Science and Technology.\n2The University of Sydney.\n(a) Code (FunSearch, ReEvo)\n (b) Thought + Code (EoH)\n(c) Only Thought + Tree-structured (TreEvo, our work)\nFigure 1: Different forms of heuristics in LLM-driven EAs.\nadoption in financial applications. In contrast, the symbolic\nparadigm remains predominant due to its ability to generate\ninterpretable factors ( \u02c7Cernevi \u02c7cien\u02d9e and Kaba \u02c7sinskas 2024).\nSuch factors facilitate rigorous strategy validation and ro-\nbust risk management. Recent research in Genetic Program-\nming (GP) (Zhang et al. 2020) and Reinforcement Learn-\ning (RL) (Yu et al. 2023), grounded in symbolic tree struc-\ntures, continues to underscore the enduring relevance of\nsymbolic approaches. Nevertheless, these approaches suffer\nfrom an inherently large search space, driven by the infinite\ncomposability of symbols, i.e., operators and market data\nfeatures (Vie, Kleinnijenhuis, and Farmer 2020). Though\nhandcrafted heuristics leverage personal expert knowledge\nto guide the search processes, the prior knowledge is often\nrestricted and would introduce biases.\nRecent advances have explored the integration of Large\nLanguage Models (LLMs) with Evolutionary Algorithms\n(EAs) (Romera-Paredes et al. 2024), which work by itera-\ntively prompting LLMs for evolving the executable codes.arXiv:2508.16334v1  [cs.CE]  22 Aug 2025",
        "char_count": 3671
      },
      {
        "page_number": 2,
        "content": "Compared to previous symbolic approaches, this pre-train\nparadigm provides the full expressiveness. Thus, the search\nspace is no longer limited to manually predefined sym-\nbols (Merler et al. 2024). The massive prior knowledge en-\ncoded in LLMs allows for the generation of diverse, context-\nrelevant code structures that handcrafted heuristics often fail\nto capture (Jiang et al. 2025). Consequently, this leads to\nfaster convergence and greater diversity in possible solu-\ntions. More recently, research has expanded to jointly evolve\nboth the underlying thoughts and the generated codes (Liu et\nal. ). This shift from code-only to thought-and-code evolu-\ntion marks an important step toward more cognitively in-\nspired and effective LLM-driven program synthesis.\nSuch a framework is expected to foster a more stable\nand efficient search process of symbolic approaches in alpha\nmining. However, existing methods mostly focus on the en-\nhancement of LLM-based evolutionary operators, overlook-\ning the underlying conceptual structure of thoughts. Tradi-\ntional symbolic approaches utilize explicit tree-based hierar-\nchies. These hierarchies naturally reflect the compositional\nand layered structure of solutions, enhancing both clarity\nand modularity. In constrast, current LLM-driven methods\nmostly generate plain-text thoughts, which constrains the ca-\npacity to represent complex relationships.\nIn this paper, we propose Tree-structured thought Evolu-\ntion (TreEvo), a novel paradigm where the evolution process\nis performed on thoughts represented as hierarchical tree\nstructures. Each sub-tree encapsulates a sub-component of\nreasoning, and higher-level nodes form increasingly abstract\nheuristics. With this representation, the tailored semantic-\nlevel evolutionary operators are also proposed for evolving\nthe tree-structures at the thought level. Empirical compar-\nisons with 7 representative methods have been conducted on\n4 stock pools across 2 diverse markets. TreEvo shows sig-\nnificant advantages over traditional symbolic approaches, in\nterms of both the predictive power of mined alphas (with\nat least 4.59% improvements) and the computational effi-\nciency (with 5x-100x acceleration). The clear superiority of\nthe proposed tree-structure and evolutionary operators are\nalso extensively analyzed by comparing with two represen-\ntative LLM-driven EAs and the ablation variants.\nIn summary, our contributions are as follows.\n\u2022 We propose the tree-structured thoughts, which explicitly\norganize the reasoning idea of alpha mining in a hierar-\nchical manner.\n\u2022 We propose TreEvo for alpha mining, a novel paradigm\nwhere evolution operates solely at the tree-structured\nthoughts level, with the novel semantic-level evolutionary\noperator.\n\u2022 We empirically show that TreEvo, as well as the way of\ncode generations via LLM-driven EAs, can be a revolu-\ntionary means for alpha mining.\nBackground and Related Works\nAlpha Mining\nAlpha mining refers to the automated discovery of predictive\nsignals (i.e., alphas) that forecast future asset returns. Al-phas form the foundation of quantitative investment strate-\ngies. The field was initially dominated by handcrafted de-\nsigns (Fama and French 1993; Grinold and Kahn 2000).\nAs markets became more complex and handcrafted features\nshowed limitations, researchers shifted toward data-driven\napproaches (De Prado 2018). In recent years, neural net-\nworks have been employed to represent the alphas. Deep\nlearning models such as multilayer perceptrons (Heaton,\nPolson, and Witte 2017), recurrent neural networks with at-\ntention mechanisms (Kim and Kang 2019), and graph neu-\nral networks (Qian et al. 2024) have shown strong abil-\nity to capture nonlinear and temporal patterns in finan-\ncial data. Despite their empirical success, these models of-\nten face challenges related to interpretability and overfit-\nting. As a result, there is renewed interest in symbolic-\nbased representation of alphas. Techniques like GP(Ren,\nQin, and Li 2024) and RL(Ren et al. 2024; Zhao et al.\n2025) are used to generate transparent and modular alpha\nformulaic expressions. Current trends also emphasize in-\ncorporating market constraints, improving robustness across\ndifferent regimes, and building models that balance predic-\ntive accuracy with interpretability (Gu, Kelly, and Xiu 2020;\nYeo et al. 2025).\nSymbolic Paradigm\nSymbolic paradigm constitutes a foundational approach\nwithin structured pattern discovery, enabling the identifica-\ntion of compact, composable representations that capture\nunderlying regularities in data. Unlike purely statistical or\nneural methods, symbolic approaches formulate patterns as\nexpressions drawn from a predefined syntax, such as al-\ngebraic formulas, logical clauses, or abstract syntax trees,\nthereby enabling structure-aware search over expressive hy-\npothesis space. Early works include Genetic Programming\n(Koza 1990), a foundational method for symbolic regres-\nsion, and later advancements such as the Eureqa system\n(Schmidt and Lipson 2009). These mothods have demon-\nstrated applicability across scientific modeling (Udrescu and\nTegmark 2020; Cranmer et al. 2020) and quantitative finance\n(La Malfa et al. 2021). Recent advances integrate symbolic\nreasoning with machine learning, employing differentiable\nsearch procedures (Liang et al. 2017), program induction\nframeworks (Ellis et al. 2021), and RL\u2013based expression\ngeneration (Crochepierre, Boudjeloud-Assala, and Barbe-\nsant 2022). These methods extend the expressiveness of\nsymbolic discovery while retaining structural interpretabil-\nity. As structured pattern discovery increasingly intersects\nwith scientific domains and high-stakes decision systems,\nsymbolic paradigms offer a principled alternative to black-\nbox learning (Wang et al. 2023).\nLLM-driven EAs\nThe recent emergence of LLMs has provided novel oppor-\ntunities to enhance EAs by leveraging their advanced capa-\nbilities in natural language understanding, symbolic reason-\ning, and generative modeling (Zhang et al. 2023; Romera-\nParedes et al. 2024). Conventional EAs typically employ\nfixed, manually designed genetic search operators such as\ncrossover and mutation, which may limit their adaptability",
        "char_count": 6209
      },
      {
        "page_number": 3,
        "content": "and efficiency when navigating complex, high-dimensional\nsearch space. Incorporating LLMs facilitates the develop-\nment of dynamic, context-sensitive evolutionary operators\nthat better capture problem-specific structures and heuris-\ntics (Hao, Zhang, and Zhou 2024; Huang et al. 2024). Pre-\nvious studies have demonstrated the efficacy of LLMs in\ngenerating potential solutions and guiding search trajecto-\nries by harnessing implicit knowledge encoded during pre-\ntraining (Tian et al. 2025; Ye et al. 2024). Furthermore,\nLLMs have been utilized to enhance representation learn-\ning within EAs, enabling richer encodings of individuals\nand constraints that improve search performance (Liu et al.\n2024). Recent advances also investigate automated evolu-\ntionary operators design through LLM-driven program syn-\nthesis and meta-optimization, reducing the reliance on hu-\nman expertise and improving generalization across diverse\nproblem domains (Lange, Tian, and Tang 2024). Despite\nthese advances, challenges remain regarding the computa-\ntional consumption of integrating LLM inference with itera-\ntive EA processes, as well as ensuring the robustness and in-\nterpretability of LLM-augmented evolutionary frameworks.\nNevertheless, the integration of LLMs into EAs constitutes\na promising and rapidly developing research direction with\nsignificant potential to advance metaheuristic optimization.\nMethod\nMain Idea\nTreEvo encodes candidate alphas as tree-structured\nthoughts. Each sub-tree denotes a logically coherent\nsub-component of the thought. Higher-level nodes are\nhierarchically composed of thoughts encoded in lower-level\nnodes. This hierarchical formulation reflects the structural\norganization of human cognition. Cognitive processes\nare widely understood to proceed from abstract, global\nconcepts toward more concrete, fine-grained elaborations.\nBy aligning the architecture of thought representation with\nthe paradigm, TreEvo enables principled manipulation\nof reasoning structures. In particular, the tree structure\nsupports the application of evolutionary operators, such as\ncrossover and mutation, at different well-defined levels of\nsemantic granularity.\nTreEvo facilitates structured search over interpretable\nand composable units of thought. The tree structure also\nachieves fine-grained control over the modification and re-\ncombination of thought components, which consequently\nenhances both search efficiency and interpretability. More-\nover, these tree-based representations provide an efficient\nfoundation for tracing, changing, and reusing intermediate\nreasoning steps. Such capabilities are essential for ensuring\ntransparency and enabling efficient structured pattern dis-\ncovery, particularly in high-stakes domains like finance.\nTree-structured Thought Representation\nThe structure shown here is an example of a hierarchical tree\nrepresentation used to organize complex concepts or compu-\ntations. At the highest level, a root node represents the over-\nall entity or idea, which is decomposed into several child\nnodes capturing its main reasoning steps.V olume Weighted\nClose-to-Open Return\nCalculate\nClose-to-Open Return\nUse\nClose priceUse\nOpen priceWeight by\nV olume\nUse\nV olumeMultiplied\nby Return\nFigure 2: An example of tree-structured thoughts.\nEach child node can further branch into its own children,\nreflecting more finer-grained operations that collectively de-\nfine the meaning of their parent node. This layer-wise de-\ncomposition continues recursively until the leaf nodes that\nrepresent the most basic, indivisible units in the structure.\nThe edges between nodes indicate the compositional re-\nlationships, where parent nodes depend on or aggregate in-\nformation from their children. The hierarchical levels corre-\nspond to different degrees of abstraction, with higher levels\nsummarizing or coordinating the details specified at lower\nlevels.\nOn this basis, TreEvo is significantly different from\nthe well-known Chain-of-Thought(CoT) (Wei et al. 2022)\nand Tree-of-Thought(ToT) (Yao et al. 2023). CoT treats a\nthought as a sequence of reasoning steps. ToT also organizes\na thought as a sequence of reasoning steps, and integrates\nmultiple related thoughts as a tree. That is, sibling reason-\ning steps under the same sub-tree typically share the same\nreasoning prefix. In contrast, TreEvo regards a whole tree\nas a thought. Consequently, the generation of the thoughts\nare also different. In CoT, the whole thought is generated by\nconstructively appending reasoning steps. In ToT, the gen-\neration of a thought is to find the optimal complete branch\nfrom the root to one leaf. In TreEvo, we need to search the\nspace of tree-structured representations, where EA can be\nadopted.\nSemantic-level Evolutionary OOperators\nLet a Thought be represented as a tree T= (V, E), where\nVis the set of reasoning units (e.g., logical steps), and E\u2286\nV\u00d7Vdefines the hierarchical dependencies between them.\nEach node vi\u2208V,i\u2208 {0, ...,|V|}, corresponds to a distinct\nsemantic or computational subcomponent, and the root node\nv0encodes the expression of the overall alpha factor.\nThe evolutionary operators are described as follows.\n\u2022Crossover : Given two thoughts T1andT2, we define\ncrossover as subtree exchange:\nT\u2032=T1\u2295vi,vjT2\nwhere vi\u2208 T 1,vj\u2208 T 2.\u2295denotes the substitution of\nviandvj. This operator preserves semantic validity and\nallows meaningful recombination of reasoning units.\nPrompt sketch: Crossover refer to two thought trees and\ngenerate a new thought tree that synthesizes reasoning\ncomponents from both parents while maintaining overall\ncoherence.",
        "char_count": 5583
      },
      {
        "page_number": 4,
        "content": "\u2022Mutation : Mutation operates Tby selecting a node vk\nand replacing its subtree with a newly generated, syntac-\ntically valid and semantically plausible tree eT:\nT\u2032=T \u2296vkeT\nwhere \u2296denotes the sub-tree replacement at node vk.\nThis allows localized alteration of logical steps while pre-\nserving the overall structure and intent of the thought.\nPrompt sketch: Mutation transforms a thought tree into a\nnew thought by introducing meaningful changes that mak-\ning it different from the Parent tree.\n\u2022Pruning : It operates by identifying and removing struc-\nturally redundant or semantically neutral sub-trees from\nT. Given the sub-tree of node vk, if it satisfies the redun-\ndancy criteria, it is replaced with a simpler expression bT:\nT\u2032=T \u2296vkbT\nThis operator facilitates structural simplification and en-\ncourages parsimony in the thought representation.\nPrompt sketch: Pruning simplifies a thought tree by re-\nmoving redundant or unnecessary components, produc-\ning a more concise thought that retains the core reasoning\nidea.\nThe basic ideas of these evolutionary operators are simi-\nlar to those in GP. However, as the solution representations\nare different, the detailed implementations are thus distinct.\nUnlike GP\u2019s symbol-level manipulations, which often pro-\nduce syntactically invalid or semantically incoherent out-\nputs, semantic-level evolutionary operators operate on well-\nformed and interpretable reasoning units. This distinction\nenables greater stability and expressiveness in the evolution-\nary process, aligning more closely with the compositional\nand hierarchical characteristics of human reasoning. Fur-\nthermore, representing thoughts as hierarchical trees sup-\nports a more granular and systematic form of evolutionary\nsearch, in which intermediate reasoning components can be\nexplicitly isolated, modified, and recombined. Such struc-\ntural transparency is difficult to achieve in flat or sequential\nrepresentations, where reasoning steps remain entangled and\nless amenable to targeted manipulation.\nThe TreEvo Framework\nThe evolution process maintains a population of Ncan-\ndidate alphas, which are represented as tree-structured\nthoughts. The population are iteratively evolved with LLM.\nAt each iteration, given Nparent thoughts, Nnew offspring\nthoughts are generated by LLM with the above three op-\nerators. To evaluate those Nnew thoughts, LLM is first ap-\nplied to generate one piece of executable code based on each\nnew thought, then each code is run with the given market\ndataset to calculate the predictive performance (e.g., Infor-\nmation Correlation). After that, a new population of Ncan-\ndidate thoughts are selected among the Nparents and N\noffsprings, based on their individual evaluated performance.\nMore specific steps are described as follows:\nStep 1 Initialization: Initialize a population PofN\nthoughts by prompting LLM with Initialization Prompts.Please see Appendix for more details. Then ask LLM to\ngenerate Ncodes based the Nthoughts and do the evalu-\nation of each code, respectively.\nStep 2 Evolutionary Process: Repeat until the predefined\nstop criteria (e.g., a given time budget) are met.\nStep 2.1: Crossover: Generate a population of Nnew\nthoughts based on Pby applying the semantic-level\ncrossover operators with LLM.\nStep 2.2: Mutation: Generate a population of Nnew\nthoughts based on Pby applying the semantic-level\nmutation operators with LLM.\nStep 2.3: Pruning: Generate a population of Nnew\nthoughts based on Pby applying the semantic-level\npruning operators with LLM.\nStep 2.4: Evaluation: Generate executable codes for\n3Nindividuals and evaluate codes performance using\nthe real-market dataset with LLM.\nStep 2.5: Selection: Select Nbest individuals from 3N\nindividuals in terms of their evaluation performance to\nupdate P.\nStep 3 Output: Output the code of the best performing\nthought through the whole process.\nExperiments\nThe experimental studies focus on three key research ques-\ntions (RQs):\n\u2022RQ1: Can LLM-driven EAs really contribute as a promis-\ning way for alpha mining?\n\u2022RQ2: Which representation of alphas is better for LLM-\ndriven EAs? The solely codes, the plain-text thoughts with\ncodes, or the tree-structured thoughts?\n\u2022RQ3: Can the idea of tree-structure also be applied to ex-\nisting LLM-driven EAs? How does the evolutionary op-\nerators contribute?\nExperimental Settings\nDataset Four commonly seen stock pools, all are widely\nbenched by the institutional investors, are considered, en-\ncompassing both Chinese A-shares market and U.S. stock\nmarket:\n\u2022Chinese A-shares market: Both CSI300 and CSI500 are\nconsidered. Here, the CSI300 tracks the 300 largest and\nmost liquid A-share stocks, which represents the large-\ncapitalization (large-cap) segment of China\u2019s market. The\nCSI500 consists of 500 mid-cap and small-cap A-share\nstocks, which reflects the behaviors of a broader market\nbesides the large-cap stocks, capturing the growth poten-\ntial of more dynamic and innovative stocks.\n\u2022U.S. stock market: Both SPX and DJI are adopted. The\nSPX , or S &P500, includes 500 leading publicly traded\ncompanies in U.S., covering all major industries and re-\nflecting the overall American economic. The Dow Jones\nIndustries Average ( DJI) consists of 30 large U.S. com-\npanies, often seen as a measure of the health of the tra-\nditional American economy, with a focus on blue-chip\nstocks.",
        "char_count": 5357
      },
      {
        "page_number": 5,
        "content": "Considering about the reproducibility, we chose six\nwidely used raw data features, i.e., opening, closing, high-\nest, lowest prices (OHLC), trading volume (volume), and\nvolume-weighted average price (vwap), for the whole ex-\nperiments. All price and volume data are forward-adjusted\nfor corporate actions to ensure consistency over time. The\ndataset is split chronologically into training set (2016/01/01-\n2020/01/01), validation set (2020/01/01-2021/01/01) and\ntest set (2021/01/01-2024/01/01).\nBaselines The framework is compared with seven baseline\nmethods. Five of them are traditional symbolic approaches,\nrepresenting the state-of-the-arts of alpha mining. The re-\nmaining two are LLM-driven EAs for code generation.\n\u2022Traditional Symbolic Approaches: XGBoost (Chen and\nGuestrin 2016) is an implementation of gradient boosted\ndecision trees with regularization and second-order op-\ntimization for accurate and scalable prediction on tab-\nular data, provided by Qlib (Yang et al. 2020). Light-\nGBM (Ke et al. 2017) is a gradient boosting frame-\nwork using histogram-based binning and leaf-wise tree\ngrowth for fast and efficient training on large datasets,\nalso provided by Qlib. GP (Zhang et al. 2020) is an\nEA, iteratively searching interpretable symbolic formulas\nthrough evolutionary operators like crossover and muta-\ntion, integrated by gplearn1.AlphaGen (Yu et al. 2023)\nis an RL framework where PPO-based RL and tokenized\nReverse Polish Notation formula generation optimize\nportfolio-level performance to produce synergistic formu-\nlaic alpha sets. QFR (Zhao et al. 2025) is an RL approach\nuses a REINFORCE policy without critic, introducing\ngreedy-baseline variance reduction and information-ratio\nbased reward shaping to mine stable, interpretable alpha\nfactors.\n\u2022LLM-driven EAs: EoH (Liu et al. ) is a framework that\ncombines LLMs and EA to evolve both thoughts and ex-\necutable code, enabling efficient automatic heuristic de-\nsign. ReEvo (Ye et al. 2024) integrates LLMs with EA\nto solely generate executable codes, strengthened with re-\nflective reasoning.\nFor traditional approaches, 22 arithmetic operators are\nused for combing the above six data features (see Ap-\npendix ), while it is no need to restrict the operators for\nLLM-driven EAs given the embedded prior knowledge.\nThe hyperparameters of the traditional symbolic ap-\nproaches are all consistent with their original providers.\nFor all LLM-driven EAs, we utilize the Qwen3-Max model\n(Yang et al. 2025) to generate both thoughts and codes of\ncandidate solutions, and the population size is fixed to 10.\nEvaluation Metrics To highlight the fast convergence of\nthe proposed TreEvo, we set its time budget for obtaining the\nbest alpha to a very small number of 200 evaluations. For\nthe five traditional symbolic approaches, we borrow their\noriginal large time budgets settings. Specifically, XGBoost,\nLightGBM, AlphaGen and QFR consume at least 1000 eval-\nuations, which is 5x larger than that of TreEvo. And the time\n1https://gplearn.readthedocs.io/en/stable/intro.htmlbudget of GP is as much as 20000 evaluations, which is 100x\nlarger than TreEvo. EoH and ReEvo, the other two LLM-\ndriven EAs, share the same setting of TreEvo.\nFor all methods, once they consume the predefined time\nbudget on each training and validation datasets, their best-\nfound alphas are applied to the corresponding testing dataset\nto obtain the predictive performance on unseen data.\nTo measure the predictive performance of each algorithm,\nwe employ two metrics commonly used across the quantita-\ntive finance industries as follows:\n\u2022ICmeasures the Pearson\u2019s correlation coefficient between\ndaily predicted values Z\u2208 Rn\u00d7T(i.e., the time series\nscores of the best alpha on each of the Tdays of all nin-\nvolved stocks in the testing dataset) and the ground-truth\nfuture 5-day returns F\u2208 Rn\u00d7T. It reflects the predictive\npower of the mined alpha with Rn\u00d7T\u00d7Rn\u00d7T\u2192[\u22121,1].\nIC is also adopted as the training loss or search objective\nfor the compared algorithms.\nIC(Z,F) =1\nTTX\nt=1Pn\ni=1(Zit\u2212\u00afZt)(Fit\u2212\u00afFt)pPn\ni=1(Zit\u2212\u00afZt)2Pn\ni=1(Fit\u2212\u00afFt)2.\n\u2022Rank IC is the Spearman\u2019s rank correlation coefficent be-\ntween daily predicted values Zand future returns F. It\ncaptures monotonic relationships, making it more robust\nto outliers and scale of values than IC.\nRank IC = IC(rank( Z),rank( F)).\nFinally, the performance of an algorithm on each testing\ndataset is calculated by averaging over 5 independent runs\nof the above process, to ensure fairness and consistency.\nResults and Analysis\nComparisons with Symbolic Methods Table 1 shows\nthe performance comparisons of traditional symbolic ap-\nproaches and TreEvo on CSI300 and CSI500. Overall,\nTreEvo consistently outperforms all baselines in terms of IC,\ndemonstrating a clear advantage in capturing the magnitude\nof future returns. Compared to even the strongest baseline\non each index, TreEvo still delivers an IC improvement of\nover4.59% on CSI300 and 4.80% on CSI500, highlighting\nthe benefit of incorporating LLMs\u2019 guidance into the alpha\nmining.\nNote that, the above advantages of TreEvo are ob-\ntained with a time budget that is 5 times fewer than the\nfour training-based methods and 100 times fewer than GP.\nThis efficiency underscores the effectiveness of LLM-driven\nsearch methods in narrowing the search space with the em-\nbedded prior knowledge and facilitating faster convergence\ntoward high-quality solutions. The reduced resource de-\nmands, combined with improved predictive accuracy, posi-\ntion TreEvo as a highly practical and worth exploring frame-\nwork for alpha mining.\nOn the CSI300 dataset, TreEvo shows the strongest pre-\ndictive power in terms of IC, confirming its ability to identify",    
        "char_count": 5695
      },
      {
        "page_number": 6,
        "content": "Table 1: Comparisons to Traditional Symbolic Approaches\nCSI300 CSI500\nIC RankIC IC RankIC\nXGBoost 0.0192 0.0241 0.0173 0.0217\n(\u00b10.0021) ( \u00b10.0027) ( \u00b10.0017) ( \u00b10.0022)\nLightGBM 0.0158 0.0235 0.0112 0.0212\n(\u00b10.0012) ( \u00b10.0030) ( \u00b10.0012) ( \u00b10.0020)\nGP 0.0445 0.0673 0.0557 0.0665\n(\u00b10.0044) ( \u00b10.0058 ) (\u00b10.0117) ( \u00b10.0154)\nAlphaGen 0.0500 0.0540 0.0544 0.0722\n(\u00b10.0021) ( \u00b10.0035) ( \u00b10.0011) ( \u00b10.0017)\nQFR 0.0588 0.0602 0.0708 0.0674\n(\u00b10.022) ( \u00b10.0014) ( \u00b10.0063) ( \u00b10.0033)\nTreEvo 0.0615 0.0649 0.0742 0.0793\n(\u00b10.0153 ) (\u00b10.0173) ( \u00b10.0081 )(\u00b10.0119)\nprofitable signals that correlate well with ground-truth re-\nturns. The superiority is also consistently observed on Ran-\nkIC, except for the comparison with GP where TreEvo is\nslightly inferior. This suggests that, with a large time bud-\nget, traditional symbolic approaches can still be powerful,\nwhile the outstanding ability of LLM-driven EAs lies in the\neffective reduction of the search space with few efforts. In\nall verified cases, TreEvo remains promising and compelling\nfor alpha mining tasks, especially when the time budget is\nlimited as the real market keeps changing rapidly.\nIn Figure 3, we further depict the cumulative return of\neach method on the CSI300 index with the commonly used\nTop-50/Drop-5 trading strategy. The trading strategy selects\nthe top 50 stocks ranked by the alpha scores at each rebal-\nance date, which is set to daily. Among them, the bottom 5\nstocks are dropped at the next rebalance date and replaced\nwith 5 new stocks from the updated top-ranked stocks. Fig-\nure 3 shows the definitive superiority of TreEvo over the tra-\nditional symbolic approaches in terms of cumulative return\nunder this strategy. Except for the very short initial period,\nTreEvo maintains the leading position throughout the back-\ntest horizon. Compared to the strongest baseline, TreEvo\nachieves a 16.91% improvement in the cumulative return.\nAnd TreEvo obtains more than 100% excess return by com-\npared to the CSI300 market benchmark (i.e., the gray dash\ncurve in the figure). Moreover, as the CSI300 index contin-\nuously goes down (get more loses), the rapidly increasing\ncurve of TreEvo demonstrates that real alphas (i.e., the man-\nagement risks) that are less correlated to beta (i.e., the mar-\nket risks) have been mined.\nTo conclude, the performance of TreEvo not only shows\nits superiority over representative approaches in the quanti-\ntative finance industries, but also supports that LLM-driven\nEAs can be revolutionary tools for alpha mining. Because it\ncan automatically generate executable codes of powerful al-\nphas in minutes rather than traditionally daily, and without\nmuch human efforts in the manufacturing loop .\nComparisons with LLM-driven EAs Table 2 reports the\ncomparative results of TreEvo against two existing LLM-\ndriven EAs across all four datasets. The results demon-\nFigure 3: Cumulative rewards of traditional symbolic ap-\nproaches on CSI300.\nstrate that TreEvo generally outperforms the compared ap-\nproaches, indicating that the alpha signals discovered by our\nmethod possess stronger linear associations with future re-\nturns across different markets. More specifically, on CSI300,\nCSI500, and DJI datasets, the improvement made by TreEvo\non IC is around 20%, which is significant. While TreEvo is\nbehind EoH on the IC metric on the SPX dataset, it maybe\nbecause the outliers of the IC values therein. The evidence is\nthat TreEvo gains a 18.6%improvement on the RankIC met-\nric, which is more robust to extreme IC values and resistant\nto the scale of IC differences.\nNotably, it is observed that EoH consistently beats ReEvo\non all cases. This shows that evolving thoughts and codes\nseparately is better than solely evolving the codes. More-\nover, given TreEvo outperforms EoH, this table further sug-\ngests that evolving tree-structured thoughts is more effective\nthan linear plain-text thoughts, which verifies the major hy-\npothesis of this work.\nFigure 4 presents the convergence curves of the three\nLLM-driven EAs on four stock markets. ReEvo generally\nconverges the slowest, indicating that solely codes evolution\ndoes not adapt to the environments effectively. For the two\nthoughts-based evolution methods, they also behave differ-\nently. On CSI300, TreEvo establishes an unequivocal superi-\nority very early and thus converges to a substantially higher\nIC. While in the other three datasets, TreEvo does signifi-\ncantly beat EoH at the beginning. Nevertheless, the curves\nof TreEvo quickly goes up and even surpass EoH on CSI500\nand DJI. This also demonstrates the better adaptation ability\nof Tree-structured thoughts within the evolution loop.\nAblation Studies To clarify whether the major perfor-\nmance gain stems from the tree-structures thought or the\nsemantic-level evolutionary operators, a variant of ReEvo\nis designed by solely evolving its codes with the proposed\ntree-structured thoughts, denoted as TReEvo. By comparing\nTReEvo with ReEvo, it suffices to show how tree-structure\ncontributes to the performance, as they share the same evolu-\ntionary operators. By comparing TReEvo with the proposed\nTreEvo, the novel evolutionary operators are ablated.\nFigure 5 depicts three independent runs of ReEvo,\nTReEvo, and TreEvo on both the CSI300 and CSI500\ndatasets. TReEvo consistently outperforms ReEvo in terms",
        "char_count": 5328
      },
      {
        "page_number": 7,
        "content": "Table 2: Performance Comparisons of LLM-driven EAs\nCSI300 CSI500 SPX DJI\nIC RankIC IC RankIC IC RankIC IC RankIC\nEoH 0.0506 0.0462 0.0584 0.0653 0.0504 0.0458 0.0514 0.0546\n(\u00b10.0188) ( \u00b10.0237) ( \u00b10.0046) ( \u00b10.0106) (\u00b10.0129) (\u00b10.0020) ( \u00b10.0110) ( \u00b10.0197)\nReEvo 0.0411 0.0409 0.0426 0.0489 0.0362 0.0308 0.0368 0.0393\n(\u00b10.0057) ( \u00b10.0218) ( \u00b10.0111) ( \u00b10.0322) ( \u00b10.0132) ( \u00b10.0186) ( \u00b10.0054) ( \u00b10.0071)\nTreEvo 0.0615 0.0649 0.0742 0.0793 0.0403 0.0543 0.0601 0.0573\n(\u00b10.0153) ( \u00b10.0173) ( \u00b10.0081) ( \u00b10.0119) (\u00b10.0155) (\u00b10.2410) ( \u00b10.0099) ( \u00b10.0283)\n(a) On CSI300\n (b) On CSI500\n (c) On SPX\n (d) On DJI\nFigure 4: Visualization of the convergence curves of three LLM-EAs on four datasets.\nof IC, verifying the advantages of the tree-structured\nthoughts. And the superiority of TreEvo over TReEvo re-\nveals that the proposed semantic-level evolutionary op-\nerators are more compatible with the evolution of tree-\nstructured thoughts.\nTake a closer look at the figure, it can be observed\nthat the performance gains brought by the tree-structured\nthought (i.e., TReEvo over ReEvo) is generally larger than\nthat of the novel evolutionary operators (i.e., TreEvo over\nTReEvo). More specifically, the averaged ICs (over three\nruns) of TReEvo are 0.0546 and 0.0631 on CSI300 and\nCSI500, respectively. This leads 34.81% and34.83% over\nthat of ReEvo, which obtains the averaged ICs of 0.0405\nand 0.0468 on the two datasets. Comparatively, the aver-\naged ICs of the proposed TreEvo are 0.0601 and 0.0726\non CSI300 and CSI500, respectively. Its advantages over\nTReEvo become 10.07% and15.06%. These simple calcu-\nlations help us to quantitatively understand that the tree-\nstructured thought is more dominant than the novel evo-\nlutionary operators to improve the performance of LLM-\ndriven EAs on alpha mining. Meanwhile, the evolutionary\noperators should also be tailored to be compatible with the\nhierarchical thoughts, otherwise the improvements would be\nlimited.\nConclusions and Future Directions\nThis paper proposes a novel LLM-driven alpha mining\nparadigm based on the tree-structured thoughts, which en-\nables more expressive and hierarchical representations of\nheuristics. By leveraging the compositional nature of trees,\nour approach allows for finer-grained reasoning and modular\nreuse of intermediate thought steps, making it particularly\n(a) On CSI300\n (b) On CSI500\nFigure 5: Results of three independent runs of ReEvo,\nTReEvo and TreEvo.\nwell-suited for complex search space like formulaic alpha.\nExtensive experiments across multiple markets demonstrate\nthat our method consistently outperforms 7 state-of-the-arts\nin terms of both predictive performance and computational\nefficiency. This strongly indicates that LLM-driven EAs are\npromising and revolutionary tools for alpha mining, and sug-\ngests the tree-structured representations therein.\nThe consistently upward curves in Figure 4 indicate that\nthe search has not yet converged, implying potential for fur-\nther improvement. However, this apparent opportunity is ac-\ncompanied by a significant risk: increasing the number of\nevaluations may make the methods more prone to overfit-\nting. The avoidance of prescribing the explicit training and\nvalidation phases of LLM-driven paradigm also bring the\npractitioners a challenging trade-off: terminating the search\ntoo early may result in underdeveloped factors, while ex-\ntending it too far may lead to overfitted alphas. Future work\nshould therefore move beyond maximizing IC alone, and in-",
        "char_count": 3503
      },
      {
        "page_number": 8,
        "content": "stead incorporate more principled search mechanisms, such\nas regularizations, complexity controls, or early stopping\nheuristics, to ensure trustworthy alpha mining.\nReferences\nAbe, M., and Nakayama, H. 2018. Deep learning for fore-\ncasting stock returns in the cross-section. In Pacific-Asia\nconference on knowledge discovery and data mining , 273\u2013\n284. Springer.\nBelle, V ., and Papantonis, I. 2021. Principles and prac-\ntice of explainable machine learning. Frontiers in big Data\n4:688969.\n\u02c7Cernevi \u02c7cien\u02d9e, J., and Kaba \u02c7sinskas, A. 2024. Explainable\nartificial intelligence (xai) in finance: a systematic literature\nreview. Artificial Intelligence Review 57(8):216.\nChen, T., and Guestrin, C. 2016. Xgboost: A scalable tree\nboosting system. In Proceedings of the 22nd acm sigkdd\ninternational conference on knowledge discovery and data\nmining , 785\u2013794.\nChen, L.; Pelger, M.; and Zhu, J. 2024. Deep learning in\nasset pricing. Management Science 70(2):714\u2013750.\nCranmer, M.; Sanchez Gonzalez, A.; Battaglia, P.; Xu, R.;\nCranmer, K.; Spergel, D.; and Ho, S. 2020. Discover-\ning symbolic models from deep learning with inductive bi-\nases. Advances in neural information processing systems\n33:17429\u201317442.\nCrochepierre, L.; Boudjeloud-Assala, L.; and Barbesant, V .\n2022. Interactive reinforcement learning for symbolic re-\ngression from multi-format human-preference feedbacks. In\nIJCAI , 5900\u20135903.\nDe Prado, M. L. 2018. Advances in financial machine learn-\ning. John Wiley & Sons.\nEllis, K.; Wong, C.; Nye, M.; Sabl \u00b4e-Meyer, M.; Morales,\nL.; Hewitt, L.; Cary, L.; Solar-Lezama, A.; and Tenenbaum,\nJ. B. 2021. Dreamcoder: Bootstrapping inductive program\nsynthesis with wake-sleep library learning. In Proceedings\nof the 42nd acm sigplan international conference on pro-\ngramming language design and implementation , 835\u2013850.\nFama, E. F., and French, K. R. 1993. Common risk fac-\ntors in the returns on stocks and bonds. Journal of financial\neconomics 33(1):3\u201356.\nGiglio, S.; Kelly, B.; and Xiu, D. 2022. Factor models,\nmachine learning, and asset pricing. Annual Review of Fi-\nnancial Economics 14(1):337\u2013368.\nGrinold, R. C., and Kahn, R. N. 2000. The efficiency\ngains of long\u2013short investing. Financial Analysts Journal\n56(6):40\u201353.\nGu, S.; Kelly, B.; and Xiu, D. 2020. Empirical asset pric-\ning via machine learning. The Review of Financial Studies\n33(5):2223\u20132273.\nHao, H.; Zhang, X.; and Zhou, A. 2024. Large language\nmodels as surrogate models in evolutionary algorithms: A\npreliminary study. Swarm and Evolutionary Computation\n91:101741.Heaton, J. B.; Polson, N. G.; and Witte, J. H. 2017. Deep\nlearning for finance: deep portfolios. Applied Stochastic\nModels in Business and Industry 33(1):3\u201312.\nHuang, B.; Wu, X.; Zhou, Y .; Wu, J.; Feng, L.; Cheng, R.;\nand Tan, K. C. 2024. Exploring the true potential: Evaluat-\ning the black-box optimization capability of large language\nmodels. arXiv preprint arXiv:2404.06290 .\nJiang, J.; Wang, F.; Shen, J.; Kim, S.; and Kim, S. 2025. A\nsurvey on large language models for code generation. ACM\nTransactions on Software Engineering and Methodology .\nJiang, W. 2021. Applications of deep learning in stock mar-\nket prediction: recent progress. Expert Systems with Appli-\ncations 184:115537.\nKe, G.; Meng, Q.; Finley, T.; Wang, T.; Chen, W.; Ma, W.;\nYe, Q.; and Liu, T.-Y . 2017. Lightgbm: A highly efficient\ngradient boosting decision tree. Advances in neural infor-\nmation processing systems 30.\nKim, S., and Kang, M. 2019. Financial series prediction\nusing attention lstm. arXiv preprint arXiv:1902.10877 .\nKoza, J. R. 1990. Genetic programming: A paradigm for\ngenetically breeding populations of computer programs to\nsolve problems , volume 34. Stanford University, Department\nof Computer Science Stanford, CA.\nLa Malfa, G.; La Malfa, E.; Belavkin, R.; Pardalos, P. M.;\nand Nicosia, G. 2021. Distilling financial models by\nsymbolic regression. In International Conference on Ma-\nchine Learning, Optimization, and Data Science , 502\u2013517.\nSpringer.\nLange, R.; Tian, Y .; and Tang, Y . 2024. Large language\nmodels as evolution strategies. In Proceedings of the Ge-\nnetic and Evolutionary Computation Conference Compan-\nion, 579\u2013582.\nLiang, C.; Berant, J.; Le, Q.; Forbus, K. D.; and Lao, N.\n2017. Neural symbolic machines: Learning semantic parsers\non freebase with weak supervision. In Proceedings of the\nConference of the 55th Annual Meeting of the Association\nfor Computational Linguistics (ACL 2017) , 23\u201333.\nLiu, F.; Tong, X.; Yuan, M.; Lin, X.; Luo, F.; Wang, Z.; Lu,\nZ.; and Zhang, Q. Evolution of heuristics: Towards efficient\nautomatic algorithm design using large language model. In\nProceedings of the 41st International Conference on Ma-\nchine Learning , ICML\u201924, 32201 \u2013 32223.\nLiu, S.; Chen, C.; Qu, X.; Tang, K.; and Ong, Y .-S. 2024.\nLarge language models as evolutionary optimizers. In 2024\nIEEE Congress on Evolutionary Computation (CEC) , 1\u20138.\nIEEE.\nMerler, M.; Haitsiukevich, K.; Dainese, N.; and Marttinen,\nP. 2024. In-context symbolic regression: Leveraging large\nlanguage models for function discovery. In Proceedings of\nthe 62nd Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 4: Student Research Workshop) ,\n589\u2013606.\nQian, H.; Zhou, H.; Zhao, Q.; Chen, H.; Yao, H.; Wang, J.;\nLiu, Z.; Yu, F.; Zhang, Z.; and Zhou, J. 2024. Mdgnn: Multi-\nrelational dynamic graph neural network for comprehensive\nand dynamic stock investment prediction. In Proceedings of",
        "char_count": 5450
      },
      {
        "page_number": 9,
        "content": "the AAAI Conference on Artificial Intelligence , volume 38,\n14642\u201314650.\nRen, T.; Zhou, R.; Jiang, J.; Liang, J.; Wang, Q.; and Peng,\nY . 2024. Riskminer: Discovering formulaic alphas via risk\nseeking monte carlo tree search. In Proceedings of the 5th\nACM International Conference on AI in Finance , 752\u2013760.\nRen, W.; Qin, Y .; and Li, Y . 2024. Alpha mining and en-\nhancing via warm start genetic programming for quantitative\ninvestment. arXiv preprint arXiv:2412.00896 .\nRomera-Paredes, B.; Barekatain, M.; Novikov, A.; Balog,\nM.; Kumar, M. P.; Dupont, E.; Ruiz, F. J.; Ellenberg, J. S.;\nWang, P.; Fawzi, O.; et al. 2024. Mathematical discoveries\nfrom program search with large language models. Nature\n625(7995):468\u2013475.\nSchmidt, M., and Lipson, H. 2009. Distilling free-form\nnatural laws from experimental data. Science 324(5923):81\u2013\n85.\nTian, H.; Han, X.; Wu, G.; Guo, A.; Zhou, Y .; Zhang, J.;\nLi, S.; Wei, J.; and Zhang, T. 2025. An llm-empowered\nadaptive evolutionary algorithm for multi-component deep\nlearning systems. In Proceedings of the AAAI Conference\non Artificial Intelligence , volume 39, 20894\u201320902.\nUdrescu, S.-M., and Tegmark, M. 2020. Ai feynman: A\nphysics-inspired method for symbolic regression. Science\nadvances 6(16):eaay2631.\nVie, A.; Kleinnijenhuis, A. M.; and Farmer, D. J. 2020.\nQualities, challenges and future of genetic algorithms: a lit-\nerature review. arXiv preprint arXiv:2011.05277 .\nWang, H.; Fu, T.; Du, Y .; Gao, W.; Huang, K.; Liu, Z.; Chan-\ndak, P.; Liu, S.; Van Katwyk, P.; Deac, A.; et al. 2023. Sci-\nentific discovery in the age of artificial intelligence. Nature\n620(7972):47\u201360.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-thought\nprompting elicits reasoning in large language models. Ad-\nvances in neural information processing systems 35:24824\u2013\n24837.\nYang, X.; Liu, W.; Zhou, D.; Bian, J.; and Liu, T.-Y . 2020.\nQlib: An ai-oriented quantitative investment platform.\nYang, A.; Li, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.;\nYu, B.; Gao, C.; Huang, C.; Lv, C.; et al. 2025. Qwen3\ntechnical report. arXiv preprint arXiv:2505.09388 .\nYao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T.; Cao, Y .;\nand Narasimhan, K. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. Advances in\nneural information processing systems 36:11809\u201311822.\nYe, H.; Wang, J.; Cao, Z.; Berto, F.; Hua, C.; Kim, H.; Park,\nJ.; and Song, G. 2024. Reevo: Large language models as\nhyper-heuristics with reflective evolution. Advances in neu-\nral information processing systems 37:43571\u201343608.\nYeo, W. J.; Van Der Heever, W.; Mao, R.; Cambria, E.; Sata-\npathy, R.; and Mengaldo, G. 2025. A comprehensive review\non financial explainable ai. Artificial Intelligence Review\n58(6):1\u201349.\nYu, S.; Xue, H.; Ao, X.; Pan, F.; He, J.; Tu, D.; and He,\nQ. 2023. Generating synergistic formulaic alpha collec-tions via reinforcement learning. In Proceedings of the 29th\nACM SIGKDD Conference on Knowledge Discovery and\nData Mining , 5476\u20135486.\nZhang, T.; Li, Y .; Jin, Y .; and Li, J. 2020. Autoalpha:\nan efficient hierarchical evolutionary algorithm for mining\nalpha factors in quantitative investment. arXiv preprint\narXiv:2002.08245 .\nZhang, F.; Mei, Y .; Nguyen, S.; and Zhang, M. 2023. Survey\non genetic programming and machine learning techniques\nfor heuristic design in job shop scheduling. IEEE Transac-\ntions on Evolutionary Computation 28(1):147\u2013167.\nZhao, J.; Zhang, C.; Qin, M.; and Yang, P. 2025. Quant-\nfactor reinforce: mining steady formulaic alpha factors with\nvariance-bounded reinforce. IEEE Transactions on Signal\nProcessing .",
        "char_count": 3645
      }
    ],
    "total_chars": 44441
  }
]

>>>> Turn 4 <<<<
...Agent is thinking...

✅ Agent determined the task is complete.